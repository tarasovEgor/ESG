#+STARTUP: latexpreview
#+TITLE: Разработка системы для автоматического сбора, анализа и визуализации информации по этичности компаний
#+AUTHOR: Соломатин Роман Игоревич
#+LANGUAGE: ru
#+cite_export: biblatex
#+OPTIONS: toc:nil H:4 ':t
#+COMMENT: ':t for https://stackoverflow.com/questions/15097114/how-to-get-smart-quotes-on-org-mode-export
#+LATEX_CLASS: HSEUniversity
#+LATEX_CLASS_OPTIONS: [PI, VKR]
#+LATEX_HEADER_EXTRA: \supervisor{к.т.н.}{доцент кафедры информационных технологий в бизнесе НИУ ВШЭ-Пермь}{А. В. Бузмаков}
#+LATEX_HEADER_EXTRA: \include{abstract_header}


* Введение
:PROPERTIES:
:UNNUMBERED: t
:END:
Эффективность работы компаний зависит от многих факторов, в том числе и от этики делового поведения и ответственности их сотрудников. Компании, которые придерживаются высоких стандартов этики и интегрируют их в свою культуру, обычно имеют более лояльных клиентов и успешнее конкурируют на рынке[cite:@mure_esg_2021]. Кроме того, соблюдение этических норм и принципов способствует укреплению репутации компании, что может привести к привлечению талантливых сотрудников и установлению долгосрочных партнерских отношений с другими компаниями и организациями. В целом, этика делового поведения играет важную роль в формировании имиджа компании и ее успеха на рынке.

В данной работе под этикой будет пониматься нацеленность компаний на принятия каких-то действий, которые краткосрочно не обязательно выигрышных для бизнеса, но которые увеличивают лояльность клиентов. Например, у клиента банка задержали зарплату и он не делает платеж по кредиту. Формально банк может по кредитному договору назначить штраф за неисполнение клиентом обязательств, но войдя в положение клиента, банк может не назначить или отменить такой штраф.

В настоящее время существует несколько сервисов, которые призваны оценивать этику компании на основании финансовых показателей[fn:1] и судебных дел[fn:2]. Для обеспечения работоспособности этих сервисов, специальные сотрудники должны посвящать большее количество времени для исследования разных компаний, чтобы определить насколько они этичны. Одним из доступных источников оценки этики являются отзывы, которые оставляют пользователи и описывают положительный и отрицательный опыт взаимодействия с компанией. Это часто включает в себя просмотр отзывов с различных веб-сайтов, что может занять много времени.

Таким образом из публичной информации можно получить этичность компании, но это занимает много времени.

Для решения этой проблемы требуется реализовать систему, которая собирает и анализирует отзывы потребителей с различных веб-сайтов, чтобы дать более полную и точную оценку этической практики компании. Собранные данные анализируются с помощью методов обработки естественного языка и машинного обучения, для выявления закономерностей и тенденций, связанных с этической практикой компании. Полученный анализ может быть использован для разработки надежной и достоверной системы оценки этичности компаний.

Объект исследования -- оценка этичности компаний.

Предмет исследования -- автоматизация оценки этичности компаний на основании отзывов клиентов.

Цель работы -- создание системы для автоматической оценки этичности компаний.

Исходя из поставленной цели, необходимо:
1. Провести анализ оценки этичности и требований.
2. Реализовать систему для оценки этичности.
3. Провести тестирование системы.

Этап анализа должен включать:
1. Анализ проблемы предметной области.
2. Анализ требований к системе.
3. Анализ существующих алгоритмов.

Этап проектирования должен включать:
1. Проектирование серверной части.
2. Проектирование модели для определения этичности.
3. Проектирование клиентской части приложения.

Этап реализации должен включать:
1. Сбор данных.
2. Обучение моделей.
3. Реализации серверной части.

Этап тестирования должен включать:
1. Тестирование модели.
2. Тестирование серверной части.
3. Тестирование клиентской части.

В ходе выполнения анализа, проектирования и реализации приложения используется объектно-ориентированный подход. Результаты анализа и решения задач проектирования формализуются с помощью диаграмм =UML=.
* Анализ предметной области
В данной главе представлен аналитический обзор оценок этичности компаний и алгоритмов машинного обучения, а также обзор существующих программных решений для поставленной проблемы.

Анализ предметной области следует разделить на следующие пункты:
1. Анализ процесса определения этичности компаний сейчас позволяет понять, как этот процесс происходит и как его лучше всего автоматизировать.
2. Анализ оценок этичности компаний для того, чтобы в дальнейшем определить этичность компаний.
3. Анализ существующих решений выполняется с целью выделения их сильных и слабых сторон по отношению к решаемой проблеме.
4. Анализ алгоритмов обработки естественного языка позволяет понять с помощью каких алгоритмов можно найти полезную информацию в текстах.
5. Анализ требований к системе позволит выделить функциональные и нефункциональные требования.
** Анализ определения этичности компании
Этичность компаний уже давно вызывает широкий общественный интерес, особенно их поведение в спорных ситуациях и предоставление услуг, ориентированных на клиента. В последние годы все большее внимание уделяется оценке этичности компаний\nbsp{}[cite:@mure_esg_2021; @semenko_korporativnaya_2022; @kudryavceva_korporativnosocialnaya_2016], особенно в банковском секторе и через призму экологических, социальных и управленческих факторов (ESG). Необходимость в таких оценках становится все более острой по мере того, как общество продолжает бороться с последствиями неправомерных действий корпораций и более широким воздействием корпоративной деятельности на общество и окружающую среду.

Сейчас процесс поиска этичной компании выглядит следующим образом: сначала ищутся компании, которые предоставляют желаемые услуги. Далее они изучаются, чтобы определить их этичность. Этот процесс включает в себя:
1. Просмотр отчетности компании.
2. Анализ ее финансовой деятельности.
3. Изучение информации о социальной ответственности.

Для этого клиенты компаний обращаются к различным источникам информации, таким как веб-сайты компаний, рейтинговые агентства, исследовательские организации и другие источники. Потом, изучаются социальные сети компании или отзывы пользователей на разных сайтах, форумах и социальных сетях, чтобы получить дополнительную информацию и оценить общее мнение о компании. После изучения каждой компании люди выбирают ту, которую они считают наиболее этичной и социально ответственной. Блок-схема данного поиска рис.\nbsp{}[[ref:fig:as_is]]. Важным фактором для определения этичности компании может быть ее социальная ответственность, устойчивость бизнеса и соблюдение норм и стандартов в области финансовой деятельности.

В целом, процесс поиска компаний и определения их этичности может быть длительным и требует серьезного подхода. Люди могут использовать различные источники информации, чтобы сделать осознанный выбор и инвестировать свои деньги в компанию, которая соответствует их ожиданиям и требованиям.
#+begin_src mermaid :file img/mermaid/as_is.png :results output :theme neutral  :exports none
%%{
   init: {
     "theme": 'base',
     "themeVariables": {
       "primaryColor": '#FFF',
       "primaryTextColor": '#000',
       "primaryBorderColor": '#000',
       "lineColor": '#000'
     }
   }
}%%
flowchart TD
    A[Поиск компаний, которые предоставляют желаемые услуги]
    A --> B[Составление списка компаний, которые предоставляют услуги]
    B --> C
    subgraph search[ ]
        C{{Изучение каждой компании}}
        C --> E[Изучение соцсетей компании]
        E --> F[Просмотр отзывов на разный сайтах]
        F --> C
    end
    F --> G[Выбор компании]
#+end_src

#+NAME: fig:as_is
#+CAPTION: Диаграмма того, как сейчас происходит поиск компании
#+ATTR_LATEX: :width 0.6\textwidth :placement [h]
[[file:img/mermaid/as_is_drawio.png]]

** Анализ оценок этичности компаний
Оценка этики компании -- это не одноразовый процесс, а скорее длительный и непрерывный процесс, который позволяет понять и оценить действия, политику и практику компании с течением времени. Оценка включает в себя рассмотрение соблюдения компанией отраслевых этических стандартов и передовой практики, а также мониторинг любых изменений в этической позиции компании с течением времени. Кроме того, участие в диалоге с компанией и консультации с организациями, специализирующимися на оценке корпоративной ответственности могут дать ценную информацию об этических практиках компании.

Компаниям важно оставаться этичными, так как в долгосрочной перспективе это приносит большую прибыль и улучшает показатели бизнеса, чем неэтичный способ ведение бизнеса[cite:@climent_ethical_2018; @mure_esg_2021]. Насколько этична компания можно рассматривать с двух сторон, самой компании и их клиентов. Со стороны компаний можно выделить факторы, которые можно получить из их отчетности:
- размер капитала, чтобы они не могли обанкротиться.
- влияние на окружающую среду.
- куда идут инвестиции\nbsp{}[cite:@harvey_ethical_1995].
Для пользователей одними из ключевых факторов можно выделить:
- качество пользовательского сервиса\nbsp{}[cite:@brunk_exploring_2010], как правило пользователи оставляют отзывы на сайтах по пятибалльной шкале.
- насколько навязчивы услуги компании\nbsp{}[cite:@mitchell_bank_1992], как правило пользователи оставляют отзывы на сайтах по пятибалльной шкале.

В данной работе этичность компаний будет определяться по отзывам клиентов, которые могут в которых содержатся проблемы качества услуг и качество сервиса.
** Анализ существующих решений
Существует несколько индексов, предназначенных для измерения этичности -- индекс Dow Jones Sustainability Indices\nbsp{}(DJSI)\nbsp{}[cite:@lopez_sustainable_2007] и FTSE4GOOD\nbsp{}[cite:@collison_financial_2008].

DJSI оценивает показатели устойчивости компаний различных секторов на основе экономических, экологических и социальных критериев. Компании отбираются на основе их показателей по сравнению с аналогичными компаниями в том же секторе. Процесс оценки включает в себя тщательную оценку компаний по различным критериям, включая корпоративное управление, экологический менеджмент, трудовую практику, права человека и социальные вопросы.

Аналогичным образом, индекс FTSE4GOOD предназначен для оценки деятельности компаний, которые демонстрируют эффективную практику экологического, социального и управленческого менеджмента (ESG). Компании отбираются на основе их практики ESG и оцениваются по различным критериям, включая изменение климата, права человека и корпоративное управление.

Индексы DJSI и FTSE4GOOD разработаны для того, чтобы помочь инвесторам определить компании, которые привержены этической практике. Эти индексы предоставляют инвесторам стандартизированный способ сравнения компаний на основе их показателей. Это помогает инвесторам принимать более обоснованные инвестиционные решения и побуждает компании внедрять устойчивую практику для привлечения инвестиций.

Для российских компаний нет аналогичных индексов. Сейчас данные об этичности компаний можно получить из агрегаторов отзывов и отчётности. Агрегаторы позволяют собрать информацию о клиентском обслуживании, а отчетность компаний о положении дел в целом. Но сейчас не существует способов, как можно оценить все вместе.
** Анализ требований к системе
Исходя из интервью с заказчиком система должна уметь:

1. Показывать историю изменений индекса с возможностью фильтровать по:
   1. Годам.
   2. Отраслям компаний, с возможностью множественного выбора.
   3. Компаниям, с возможностью множественного выбора.
   4. Моделям, с возможностью множественного выбора.
   5. Источникам, с возможностью множественного выбора.
2. Агрегировать значения индекса по годам и кварталам.
3. Анализировать текстовые отзывы для построения индекса этичности на основании позитивности или негативности отзывов.
4. Иметь возможность добавления анализа текста несколькими вариантами.
5. Сохранять тексты для последующего анализа другими методами.
6. Система должна собирать данные с сайтов banki.ru, sravni.ru и комментарии из групп "вконтаке".

На основе описания функциональных требований была создана диаграмма вариантов использования, которая представлена на рисунке\nbsp{}[[ref:fig:usecasefull]].
#+NAME: fig:usecasefull
#+CAPTION: Диаграмма вариантов использования
#+ATTR_LATEX: :placement [h!] :width \textwidth
[[file:img/use-case.png]]

Также были получены нефункциональные требования:
1. Построение графика не должно занимать больше секунды.
2. Сбор данных должен происходить автоматически.
3. Данные должны обрабатываться автоматически.
4. Система должны способна работать с большим объемом информации. Несколько гигабайт текста.
5. Система должна быть стабильной и надежной, обеспечивая непрерывную работу без сбоев или перебоев.
** Анализ метрик классификации
Исходя из собранных требований в данной работе будет решаться задача классификации. Для определения качества работы алгоритма будут рассмотрены несколько метрик:
1. Доля правильных ответов
2. Точность
3. Полнота
4. F-мера

Для лучшего понимания этих метрик рассмотрим матрицу ошибок. Матрица ошибок (см. таблицу [[ref:tbl:confusion_matrix]]) является таблицей, которая показывает количество верно и неверно классифицированных примеров для каждого класса. Она состоит из четырех значений:
- Истинно-положительные (True Positives, TP): количество примеров, которые были правильно классифицированы как положительные.
- Истинно-отрицательные (True Negatives, TN): количество примеров, которые были правильно классифицированы как отрицательные.
- Ложно-положительные (False Positives, FP): количество примеров, которые были неправильно классифицированы как положительные.
- Ложно-отрицательные (False Negatives, FN): количество примеров, которые были неправильно классифицированы как отрицательные.

#+NAME: tbl:confusion_matrix
#+CAPTION: Матрица ошибок
#+ATTR_LATEX: :align |c|c|c| :placement [h!]
|--------+---------------------+---------------------|
|        | y=1                 | y=0                 |
|--------+---------------------+---------------------|
| f(x)=1 | True Positives, TP  | False Positives, FP |
|--------+---------------------+---------------------|
| f(x)=0 | False Negatives, FN | True Negatives, TN  |
|--------+---------------------+---------------------|

Доля правильных ответов (Accuracy) показывает, как часто модель правильно классифицирует примеры. Она вычисляется как отношение числа верно классифицированных примеров к общему числу примеров формула [[ref:eq:accuracy]]:

#+NAME: eq:accuracy
\begin{equation}
\text{{Accuracy}} = \frac{{TP + TN}}{{TP + TN + FP + FN}}
\end{equation}

Точность (Precision) показывает, какая доля примеров, классифицированных как положительные, действительно является положительными. Она вычисляется как отношение числа истинно-положительных примеров к сумме истинно-положительных и ложно-положительных примеров формула [[ref:eq:precision]]:

#+NAME: eq:precision
\begin{equation}
\text{{Precision}} = \frac{{TP}}{{TP + FP}}
\end{equation}

Полнота (Recall) показывает, какая доля положительных примеров была правильно классифицирована. Она вычисляется как отношение числа истинно-положительных примеров к сумме истинно-положительных и ложно-отрицательных примеров формула [[ref:eq:recall]]:

#+NAME: eq:recall
\begin{equation}
  \text{{Recall}} = \frac{{TP}}{{TP + FN}}
\end{equation}

F-мера (F1 Score) является гармоническим средним между точностью и полнотой. Она позволяет учесть оба показателя и оценить баланс между ними. Лучше всего подходит для несбалансированных выборок. F-мера вычисляется по формуле [[ref:eq:f1_score]]:
#+NAME: eq:f1_score
\begin{equation}
F1 = 2 \times \frac{{\text{{Precision}} \times \text{{Recall}}}}{{\text{{Precision}} + \text{{Recall}}}}
\end{equation}

Выбор метрики для определения качества работы алгоритма будет зависеть от данных для ее обучения.
** Алгоритмы для обработки естественного языка
В требованиях было заявлено, что для оценки этичности компаний надо обрабатывать текстовые отзывы, поэтому в данной работе будут рассмотрены алгоритмы по обработке естественного языка.

Алгоритмы машинного обучения для анализа текста получили широкое распространение для извлечения информации из неструктурированных данных с помощью больших помеченных наборов данных. Среди различных используемых методов несколько алгоритмов оказались особенно эффективными в этой области. Каждый из этих алгоритмов обладает уникальными характеристиками, которые делают их хорошо подходящими для определенных задач. В этом разделе будут рассмотрены следующие алгоритмы:
   1. Мешок слов
   2. TF-IDF
   3. Word2Vec
   4. FastText
   5. BERT
*** Мешок слов
Мешок слов\nbsp{}[cite:@harris_distributional_1954] -- метод анализа, который позволяет упрощенно представить текст, как таблицу, где для каждого документа показано количество вхождений слова. В данной модели все слова предстают как множество слов без учёта грамматики и порядка. В процессе обработки текст разбивается на отдельные слова или токены, игнорируя грамматические правила и порядок слов. Затем строится словарь, содержащий все уникальные слова из текстового корпуса. Каждому слову в словаре присваивается уникальный идентификатор. После построения словаря каждый документ представляется в виде вектора, где каждый элемент вектора соответствует слову из словаря, а значение элемента -- количество вхождений этого слова в документе. Таким образом, каждый документ представляется в виде разреженного вектора, где большинство элементов равно нулю. Например, для предложений «Мама мыла раму»(1) и «Иван поломал раму»(2) результат работы алгоритма показан в таблице [[tbl:bow]]:
#+NAME: tbl:bow
#+CAPTION: Пример мешка слов
#+ATTR_LATEX: :align |c|c|c|c|c|c|
|--------+------+------+------+------+---------|
|        | мама | мыла | раму | иван | поломал |
|--------+------+------+------+------+---------|
| текст1 |    1 |    1 |    1 |    0 |       0 |
|--------+------+------+------+------+---------|
| текст2 |    0 |    0 |    1 |    1 |       1 |
|--------+------+------+------+------+---------|
*** TF-IDF
TF-IDF\nbsp{}[cite:@joneskarensparck_statistical_1972] (TF частотность слова (term frequency), IDF -- обратная частота документов (inverse document frequency)) -- статистический показатель, применяемый для оценки важности слова в контексте документа. Большой вес в TF-IDF получат слова с высокой частотой в пределах конкретного документа и с низкой частотой употреблений в других документах. В процессе TF-IDF текст разбивается на отдельные слова или токены, игнорируя грамматические правила и порядок слов.

TF -- частота слова в коллекции. Таким образом, оценивается важность слова $t_{i}$ по формуле\nbsp{}[[ref:eq:tf]]:

#+name: eq:tf
\begin{equation}
TF(t,d)=\frac{n_{t}}{\sum_{k}n_{k}},
\end{equation}
где $n_{t}$ -- число вхождений слова $t$ в документ, а в знаменателе -- общее число слов в данном документе.

IDF -- инверсия частоты, с которой некоторое слово встречается в документах коллекции. Чем реже слово встречается в документах, тем выше значение IDF и тем больше важности придается слову в контексте корпуса. Оно рассчитывается по формуле\nbsp{}[[ref:eq:IDF]].

#+name: eq:IDF
\begin{equation}
IDF(t, D)=\log\frac{|D|}{|\{d_{i}\in D\mid t \in d_{i}\}|},
\end{equation}
где $|D|$ -- число документов в коллекции, а $|{d_{i}\in D|t\in d_{i}}|$ -- число документов из коллекции $D$, в которых встречается $t$ (когда $n_{t}\neq 0$).

Таким образом, мера TF-IDF является произведением двух множителей\nbsp{}[[ref:eq:TFIDF]]:
#+NAME: eq:TFIDF
\begin{equation}
\text{TF-IDF}(t,d,D)=TF(t,d)\times IDF(t,d)
\end{equation}
*** Word2Vec
Word2Vec\nbsp{}[cite:@mikolov_distributed_2013] -- это алгоритм обработки естественного языка, который используется для получения векторных представлений (эмбеддингов) слов на основе их семантического контекста. Он работает на основе распределенного представления слов, идея которого состоит в том, что слова, встречающиеся в похожих контекстах, имеют схожие семантические значения. Алгоритм предлагает две основные архитектуры: CBOW (непрерывный мешок слов) и Skip-gram.

1. Непрерывный мешок слов:
   Архитектура CBOW состоит в обучении модели для предсказания целевого слова на основе контекстных слов. Например, для предложения "Мама мыла раму" модель CBOW пытается предсказать слово "мыла" на основе остальных слов.
2. Skip-gram:
   Архитектура Skip-gram работает в обратном направлении по сравнению с CBOW. Она предсказывает контекстные слова на основе целевого слова. То есть для слова "мыла" модель Skip-gram пытается предсказать остальные контекстные слова.

Результатом обучения Word2Vec являются векторные представления слов, где каждое слово представлено вектором фиксированной длины. Векторы слов сохраняют в себе семантическую информацию о значениях слов и их семантической близости.
*** FastText
FastText\nbsp{}[cite:@joulin_bag_2016]- это библиотека и метод машинного обучения, разработанный командой Facebook AI Research, для обработки естественного языка. Он является эффективным инструментом для работы с текстовыми данными и создания векторных представлений слов.

FastText расширяет идею Word2Vec, добавляя поддержку для обработки подслов. В отличие от Word2Vec, который работает только на уровне слов, FastText представляет слова как комбинации символьных n-грамм. N-граммы - это последовательности символов фиксированной длины, которые могут быть префиксами, суффиксами или внутренними частями слова.
*** BERT
BERT\nbsp{}[cite:@devlin_bert_2019] (Bidirectional Encoder Representations from Transformers) -- это нейросетевая языковая модель, которая относится к классу трансформеров. Она состоит из 12 «базовых блоков» (слоев), а на каждом слое 768 параметров.

На вход модели подается предложение или пара предложений. Затем разделяется на отдельные слова (токены). Потом в начало последовательности токенов вставляется специальный токен =[CLS]=, обозначающий начало предложения или начало последовательности предложений. Пары предложений группируются в одну последовательность и разделяются с помощью специального токена =[SEP]=, затем к каждому токену добавляется эмбеддинг, показывающий к какому предложению относится токен. Потом все токены превращаются в эмбеддинги (см. рисунок \nbsp{}[[fig:inputemebeddings]]) по механизму описанному в работе\nbsp{}[cite:@vaswani_attention_2017].

#+CAPTION: Пример ввода текста в модель
#+NAME: fig:inputemebeddings
#+ATTR_LATEX: :placement [h]
[[file:img/Input_Emebeddings.pdf]]

При обучении модель выполняет на 2 задания:
 1) Предсказание слова в предложении

    Это задание обучается следующим образом -- 15% случайных слов заменяются в каждом предложении на специальный токен =[MASK]=, а затем предсказываются на основании контекста. Однако иногда слова заменяются не на специальны токена, в 10% заменяются на случайный токен и еще в 10% заменяются на случайное слово.

    Поскольку стандартные языковые модели (см. рисунок \nbsp{}[[fig:BERT_comparisons]]), такие как ELMo\nbsp{}[cite:@peters_deep_2018] и GPT\nbsp{}[cite:@radford_language_2019], либо смотрят текст слева направо, либо справа налево, они не подходят для некоторых типов заданий. Однако BERT является двунаправленной моделью, что означает, что для каждого слова он может рассмотреть его контекст и использовать эту информацию для предсказания замаскированного слова.

    #+CAPTION: Сравнение принципов работы BERT, ELMo, GPT
    #+NAME: fig:BERT_comparisons
    #+ATTR_LATEX: :placement [h]
    [[file:img/BERT_comparisons.pdf]]
 2) Предсказание следующего предложения

    Для того чтобы обучить модель, которая понимает отношения предложений, она предсказывает, идут ли предложения друг за другом. Для этого с 50% вероятностью выбирают предложения, которые находятся рядом и наоборот. Пример ввода пары предложений в модель (см. рисунок\nbsp{}[[fig:bert_pretrainin]]).

    #+CAPTION: Схемам работы BERT
    #+NAME: fig:bert_pretrainin
    #+ATTR_LATEX: :width 0.6\textwidth :placement [h]
    [[file:img/bert_pretrainin.png]]
*** Выводы
В данном разделе были рассмотрены различные алгоритмы для обработки текста. Как показали исследования[cite:@gonzalez-carvajal_comparing_], каждый алгоритм может подойти к той или иной задаче, поэтому в данной работе они все будут рассмотрены.
** Алгоритмы для классификации
Для определения на сколько этична компания, каждый отзыв будет классифицироваться. Для этого будут рассмотрены следующие алгоритмы:
1. Логическая регрессия
2. Метод опорных векторов
3. Случайный лес
4. Градиентный бустинг
*** Логистическая регрессия
Логистическая регрессия[cite:@fan_liblinear_2008] является одним из наиболее распространенных алгоритмов машинного обучения, который применяется для задач классификации. Она основана на логистической функции (см. форрмулу\nbsp{}[[ref:eq:logreg]]), которая преобразует входные данные в вероятности принадлежности к определенным классам.

#+name: eq:logreg
\begin{equation}
f(z)=\frac{1}{1+\exp(-z)},
\end{equation}
где z -- скалярное произведение весов модели, на признаки ответа.
*** Метод опорных векторов
Метод опорных векторов[cite:@platt_probabilistic_2000] (Support Vector Machine, SVM) — это алгоритм машинного обучения, который используется для задач классификации и регрессии. Он основан на принципе максимизации зазора (margin) между классами объектов.

Идея SVM заключается в том, чтобы найти оптимальную гиперплоскость, которая разделяет два класса объектов в максимально возможном зазоре. Гиперплоскость представляет собой (n-1)-мерную гиперплоскость в n-мерном пространстве, где n - количество признаков.

Данная модель часто применяется для текстовой классификации[cite:@joachims_text_1998].
*** Случайный лес
Случайный лес[cite:@breiman_random_2001](Random Forest) является ансамблевым методом машинного обучения, который объединяет несколько решающих деревьев для решения задач классификации и регрессии. Каждое дерево строится независимо от других на основе различных подвыборок данных (bootstrap samples) и случайного подмножества признаков. Это позволяет деревьям быть разнообразными и уменьшает корреляцию между ними. На каждом узле дерева выбирается лучший признак и значение порога, на которое данные будут разделены на две ветви. Этот выбор делается на основе критерия информативности, такого как индекс Джини (для классификации) или среднеквадратичная ошибка (для регрессии).
*** Boosting
Градиентный бустинг[cite:@friedman_greedy_2001] (Gradient Boosting) - это алгоритм машинного обучения, который также является ансамблевым методом, но в отличие от случайного леса, градиентный бустинг строит ансамбль слабых моделей (обычно решающих деревьев) последовательно, обучая каждую модель на ошибках предыдущих моделей.

Сначала инициализируется начальная модель, которая может быть простой, например, константой или средним значением целевой переменной. Это начальное предсказание будет постепенно улучшаться в процессе построения ансамбля моделей. Затем строится первая модель. Ошибка между предсказанными значениями первой модели и фактическими значениями вычисляется с помощью функции потерь, такой как среднеквадратичная ошибка (для задач регрессии) или логистическая функция потерь (для задач классификации). После обучения первой модели вычисляется градиент функции потерь по отношению к предсказаниям первой модели. Градиент показывает, в каком направлении и насколько сильно нужно скорректировать предсказания первой модели, чтобы уменьшить ошибку.  Далее строится следующая модель, которая обучается на остатках первой модели. Остатки представляют собой разницу между фактическими значениями и предсказаниями первой модели. Вторая модель приближает остатки, чтобы улучшить предсказания

Одни из самых популярных реализаций градиентного бустинга:
1. Scikit-learn -- эта реализация предлагает простую реализации градиентного бустинга с удобным интерфейсом.
2. XGBoost[cite:@chen_xgboost_2016] -- предоставляет много дополнительных возможностей и оптимизаций, которые делают ее одной из наиболее мощных библиотек для градиентного бустинга. Обладает высокой производительностью и позволяет обучаться на GPU[cite:@mitchell_accelerating_2017], что ускоряет процесс.
3. CatBoost[cite:@prokhorenkova_catboost_2018] -- относительно новой библиотекой градиентного бустинга, разработанной компанией Yandex. Он имеет сходные функции и возможности с XGBoost, но также включает в себя некоторые уникальные возможности, которые лучше позволяют обрабатывать параметры, которые отвечают за категориальные признаки. Также имеет поддержку обучения на GPU[cite:@dorogush_catboost_2018].
*** Выводы
Все рассмотренные алгоритмы часто используется для анализа текста и не ясно, какой из них справится лучше в поставленной задаче. Поэтому в данной работе будут рассмотрены все эти алгоритмы.
** Выбор технологий для разработки
Для реализации этой системы будет использоваться язык Python[cite:@vanrossum_python_2009]. Для этого языка разработано много библиотек, которые позволят быстро реализовать алгоритмы обработки естественного языка, в частности в этом проекте будет использоваться Pytorch\nbsp{}[cite:@paszke_pytorch_2019] и HuggingFace\nbsp{}[cite:@wolf_transformers_2020]. Для реализации API будет использоваться FastAPI, что позволит разработать API для системы с автоматической генерацией документации.

Для хранения данных будет использоваться объектно-реляционная система управления базами данных PostgreSQL, что позволит обрабатывать большие объемы данных. Для работы с ней будет использоваться Code first подход, с помощью Python библиотек Sqlalchemy и Alembic для изменения схемы данных (миграций).
** Выводы по главе
По итогам анализа предметной области, можно сделать вывод о том, что определение этичности компаний является важной задачей, так как с ней сталкиваются многие люди ежедневно и тратят много времени, которую можно автоматизировать с помощью алгоритмов машинного обучения. Обзор существующих решений показал, что сейчас нет индекса, который бы учитывал мнение клиентов для анализа этичности, и может потребоваться разработка нового средства, учитывающего особенности задачи.

Изучение существующих исследований и работ показало, что в сфере обработки естественного языка применяются мешок слов, TF-IDF, Word2Vec, FastText и BERT и каждый из этих алгоритмов показал свою эффективность.

Также для задач классификации были рассмотрены: логистическая регрессия, метод опорных векторов, случайный лес и градиентный бустинг. И они будут рассмотрены в этой работе.

Путем проведения анализа требований к системе, можно определить функциональные и нефункциональные требования, которые необходимо учесть при разработке решения. На основе собранной информации составлено техническое задание@@latex:~\ref{tz_chap}@@.
* Проектирование системы
В данной главе представлена общая архитектура системы, базы данных и каждого модуля отдельно.

Этап проектирования следует разделить на следующие пункты:
1. Определение основных компонентов системы.
2. Проектирование взаимодействия компонент.
3. Проектирование базы данных и модулей для работы, обработки, сбора и агрегации данных.
4. Проектирование модели для обработки естественного текста.

Данная глава предоставляет описание системы, каждого компонента и их взаимосвязь в достижении желаемого результата.
** Создание метода для оценки этичности
Для оценки этичности компаний в данном исследовании принимается во внимание отзывы, оставленные о них, и анализируется их характер - позитивный или негативный. Изначально использовался подход, основанный на разности долей позитивных и негативных отзывов (базовый индекс, Base index). Этот метод эффективен при большом количестве отзывов, однако при небольшом объеме отзывов он не предоставляет достоверную картину.

Поскольку люди склонны писать отзывы чаще о негативном опыте, чем о положительном, среднее значение индекса обычно имеет отрицательное значение. Это нежелательно с точки зрения оценки этичности компаний. С целью преодолеть этот эффект, рассчитывается среднее значение - разность позитивных и негативных долей отзывов за год для каждого источника компании (средний индекс, Mean Index). Затем это значение вычитается из базового индекса, что приближает его к нулю. В результате компании с средним уровнем этичности будут иметь значение индекса около нуля, тогда как компании, которые проявляют более низкий или высокий уровень этичности, будут иметь отрицательное или положительное значение соответственно.

Данный подход эффективен при наличии большого числа отзывов, однако при ограниченном количестве отзывов значение индекса может быть слишком велико или слишком мало, не отражая действительное качество компаний. Для учета этого смещения было решено использовать доверительные интервалы.

Поскольку позитивность и негативность отзыва это дискретная величина, то она будет иметь распределение Бернулли[cite:@_encyclopaedia_]. Дисперсия этого распределения рассчитывается по формуле $pq$ для одной точки, а для последовательность $\frac{pq}{n}$, где $p$ вероятность 1, и $q$ вероятность 0. В данной задаче $p$ будет рассчитываться, как доля класса среди всех отзывов, а $q$ как доля остальных отзывов.

В конечном итоге для вычисления индекса средней компании используется абсолютная разность между базовым индексом и средним индексом, а затем из этого значения вычитается дисперсия (полученная формула [[ref:eq:ethics]]). Это приводит к тому, что компании с ограниченным количеством отзывов имеют большое стандартное отклонение, что означает, что их индекс будет близок к нулю или точно равен нулю. С другой стороны, у компаний с большим количеством отзывов стандартное отклонение будет меньшим, и оно либо не изменит, либо незначительно изменит их значение индекса. Таким образом, этот метод позволяет проверить, пересекается ли доверительный интервал индекса с нулевым значением. Если интервал пересекается, это означает, что значение индекса близко к нулю, а если интервал не пересекается, то берется консервативная нижняя оценка. Такой подход к вычислению индекса делает его более нейтральным и менее подверженным выбросам.

#+NAME: eq:ethics
\begin{equation}
\begin{aligned}
    \text{Base index} &= \frac{\text{positive} - \text{negative}}{\text{total}} \\
    \text{Std index} &= \sqrt{\frac{\text{positive}\cdot(\text{total-positive})}{total^{3}} + \frac{\text{negative}\cdot(\text{total - negative})}{total^{3}}} \\
    \text{Index} &=
        \begin{cases}
            \max(\text{Base Index}-\text{Mean Index}-\text{Std Index},0), & \text{Base index} > \text{Mean index} \\
            \min(\text{Base Index}-\text{Mean Index}+\text{Std Index},0), & \text{Base index} < \text{Mean index} \\
        \end{cases}
% (2 * ((index_base - index_mean) > 0) - 1) * (max(abs(index_base - index_mean) - index_std, 0))
\end{aligned}
\end{equation}


$positive$ -- количество позитивных предложений,

$negative$ -- количество негативных предложений,

$total$ -- количество предложений.
** Проектирование определения наиболее подходящей модели
Для определения наиболее подходящей модели будет использоваться набор данных, состоящий из 6,000 предложений, размеченных тремя экспертами с учетом этических практик. Однако классы предложений в этом наборе данных оказались несбалансированными, как показано на диаграмме\nbsp{}ref:fig:class_balance (с отрицательными предложениями, обозначенными "-", в большем количестве, чем положительными предложениями, обозначенными "+"). Поэтому в качестве основной метрики будет использоваться F1-мера, так как она наиболее подходит для работы с несбалансированными наборами данных. Для улучшения работы алгоритмов для определения класса каждого предложения будет использоваться наиболее часто встречающийся класс, назначенный экспертами.

#+CAPTION: Распределение классов
#+NAME: fig:class_balance
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
[[file:img/class_balance.png]]

Для обработки текста будут рассмотрены алгоритмы, результат работы которых будет подаваться на вход алгоритму кластеризации:
1. Мешок слов.
2. TF-IDF.
3. Word2Vec обученный на русском языке.
4. fastText обученный на русском языке\nbsp{}[cite:@korogodina_evaluation_2020].
5. Модификация BERT для русского языка RuBERT\nbsp{}[cite:@kuratov_adaptation_2019].
6. RuBERT дообученый на классификацию эмоций.
7. Дообученый RuBERT на собранных данных.

Перед подачей в модели данные отзывы будут предварительно очищены от цифр и ссылок, а также произведено их приведение к начальной форме (лемматизация). Для моделей, основанных на BERT, отзывы будут обрабатываться в двух вариантах: исходный текст и текст, подвергнутый обработкам.

Для определения наиболее подходящего алгоритма определения этичности будут перебираться все пары метода обработки текста и моделей для классификации. Для каждого алгоритма классификации будут подобраны оптимальные параметры, которые позволят сделать наилучшее предсказание на данных.

Для логистической регрессии будут подобраны следующие параметры:
1. С -- параметр отвечающий за силу регуляризации алгоритма. Будет подбираться от 0.00001 до 100.
2. Penalty -- параметр отвечающий за тип регуляризации l1 или l2.
3. Solver -- тип алгоритма оптимизации libliniar[cite:@fan_liblinear_2008] или saga[cite:@defazio_saga_2014].
4. @@latex:Max\_iter@@-- количество итераций для обучения. Будет подбираться от 100 до 1000.

Для метода опорных векторов будут подобраны следующие параметры:
1. С -- параметр отвечающий за силу регуляризации алгоритма. Будет подбираться от 0.00001 до 100.
2. Kernel -- тип ядра для SVM (linear, poly, rbf или sigmoid).
3. Gamma -- коэффициент ядра (scale или auto).
4. Degree -- степень полиномиального ядра. Будет подбираться от 1 до 5

Для случайного леса будут подобраны следующие параметры:
1. @@latex:N\_estimators@@ -- количество итераций для обучения. Будет подбираться от 100 до 1000.
2. @@latex:Max\_depth@@ -- глубина дерева. Будет подбираться от 1 до 10.
3. @@latex:Max\_features@@ -- количество признаков для разбиения на каждом узле (корень из количества параметров или логарифм по основанию 2).
4. Criterion -- критерий для разбиения узлов дерева (Джини или кросс-энтропия).

Для градиентного бустинга (scikit-learn) будут подобраны следующие параметры:
2. @@latex:Learning\_rate@@ -- сколько вносит каждое дерево в алгоритм. Будет подбираться от 0.00001 до 1.
3. @@latex:N\_estimators@@ -- количество итераций для обучения. Будет подбираться от 100 до 1000.
4. @@latex:Max\_depth@@ -- глубина дерева. Будет подбираться от 1 до 10.
5. @@latex:Max\_features@@ -- количество признаков для разбиения на каждом узле (корень из количества параметров или логарифм по основанию 2).

Для xgboost будут подобраны следующие параметры:
1. @@latex:N\_estimators@@ -- количество итераций для обучения. Будет подбираться от 100 до 1000.
2. @@latex:Max\_depth@@ -- глубина дерева. Будет подбираться от 1 до 10.
3. @@latex:Learning\_rate@@ -- сколько вносит каждое дерево в алгоритм. Будет подбираться от 0.00001 до 1.
4. Gamma -- минимальное уменьшение функции потерь, необходимое для создания нового разбиения на узле. Будет подбираться от 0 до 20.
5. Subsample -- доля обучающих примеров, используемых для обучения каждого дерева. Будет подбираться от 0 до 1.

Для catboost будут подобраны следующие параметры:
1. @@latex:N\_estimators@@ -- количество итераций для обучения. Будет подбираться от 100 до 1000.
2. @@latex:Max\_depth@@ -- глубина дерева. Будет подбираться от 1 до 10.
3. @@latex:Learning\_rate@@ -- сколько вносит каждое дерево в алгоритм. Будет подбираться от 0.00001 до 1.

Потом результаты работы каждого алгоритма с лучшими гиперпараметрами будут сравниваться между друг другом по F1 и так определиться лучшая модель.
** Проектирование архитектуры системы
Система будет разделена на отдельные независимые компоненты (микросервисы), что позволит ей быть надежной (если в какой-то части системы будут сбои, то остальная часть системы продолжит работать) и масштабируемой (легко добавлять новые компоненты). Каждый микросервис системы будет представлять собой docker container[cite:@merkel_docker_2014], который будет управляться с помощью docker compose. Каждый сервис будет реализовывать отдельный компонент бизнес-логики и взаимодействовать с другими компонентами через REST API.

Было выделено 4 главных компонента бизнес логики:
1. Работа с базой данных -- это HTTP API, который обеспечивает возможность сохранения и получения данных из базы данных. Данный компонент принимает запросы на сохранение данных, получение информации из базы данных и возвращает результаты обработки этих запросов.
2. Сбор данных -- компонент, который отвечает за сбор информации с нескольких источников. Для этого используется несколько независимых сборщиков данных, которые работают с различными сайтами и другими источниками.
3. Обработка данных -- данный компонент содержит несколько моделей, которые используются для анализа данных. Эти модели производят различные виды анализа, от простой фильтрации и сортировки до более сложных операций анализа и прогнозирования.
4. Агрегирование данных -- этот компонент отвечает за агрегацию обработанных данных в единый индекс. Данный индекс может быть использован для удобного представления полученных результатов в виде отчетов и графиков. Данный модуль нужен для того чтобы быстро получать новые графики, так как агрегирование всех отзывов для компаний может занимать много времени.

Результат архитектуры системы на рис.\nbsp{}[[ref:fig:architecture]].

#+NAME: fig:architecture
#+CAPTION: Диаграмма архитектуры системы
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
[[file:img/architecture.png]]

Сервис для работы с базой данных, который будет обеспечивать сохранение и получение информации из различных сервисов сбора и обработки данных. Для этого будет предоставлен API, который будет использоваться для отправки и получения данных.

Сервисы сбора данных будут отправлять собранные тексты в формате JSON на сервис работы с базой данных с помощью HTTP запросов. Кроме того, информация, необходимая для сбора данных, будет храниться в базах данных соответствующих сервисов, что соответствует принципам микросервисной архитектуры[cite:@ghofrani_challenges_2018].

Сервис агрегации данных будет периодически обновлять базу данных один раз в день для обеспечения актуальности данных.

Сервис сбора данных будет включать модель машинного обучения, которая будет использоваться для анализа данных, полученных из сервиса сбора данных. После обработки данных, результаты будут отправляться обратно в сервис сбора данных.
** Проектирование базы данных
Исходя из поставленных требований было решено разделить базу данных на 2 подчасти:
1. Основная база данных будет хранить данные.
2. База данных для агрегации будет позволять быстро получать агрегированные данные.

*** Проектирование основной базы данных
На основании требований была разработана следующая схема базы данных:

Таблица сфер компаний позволяет в дальнейшей удобно фильтровать данные в зависимости от типа компании.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица сфера компании\label{tbl:company_type}} :align colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines :position [h!]
| *Название*       | *Тип*    | *Описание*                 |
| Идентификатор  | Целое  | Уникальный идентификатор |
| Сфера компании | Строка |                          |

Таблица со списком компании будет хранить основную информации о компаниях.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица компаний\label{tbl:companies}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*           | *Тип*    | *Описание*                                                                 |
| Идентификатор      | Целое  | Уникальный идентификатор                                                 |
| Название компании  | Строка |                                                                          |
| Описание компании  | Строка | Дополнительное поле для сохранения вспомогательной информации о компании |
| Лицензия компании  | Строка | По лицензии компаний может будет сопоставлять компании на разных сайтах  |
| Код сферы компании | Целое  | Внешний ключ из таблицы Сфера компании                                   |

Аналогично для сфер компаний таблица для типов источников позволяет удобно работать с данными в дальнейшем.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица тип источников\label{tbl:source_type}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*                | *Тип*    | *Описание*                 |
| Идентификатор           | Целое  | Уникальный идентификатор |
| Название типа источника | Строка |                          |

Таблица источников будет хранить информацию об источниках и когда было последнее обновление данных для них (в полях "состояние сборщика данных" и "дата последнего сбора данных"). Поле "состояние сборщика данных" будет иметь формат json, так как для разных источников информации потребуется сохранять информацию в различном виде и сложно определить наиболее подходящий формат заранее.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица источники\label{tbl:sources}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*                  | *Тип*      | *Описание*                                                         |
| Идентификатор             | Целое    | Уникальный идентификатор                                         |
| Сайт                      | Строка   | Сайт источника                                                   |
| Код типа источника        | Целое    | Внешний ключ из таблицы тип источника                            |
| Состояние сборщика данных | JSON     | Данные о текущем состояние сборщика данных, если возникнет сбой  |
| Дата последнего сбора     | DateTime | Точка когда сбор данных закончился, для дальнейшего сбора данных |

Аналогично для сфер компаний таблица для типов модели позволяет удобно работать с данными в дальнейшем.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица тип модели\label{tbl:model_type}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*        | *Тип*    | *Описание*                 |
| Идентификатор   | Целое  | Уникальный идентификатор |
| Название модели | Строка |                          |

Таблица модели позволяет сохранять информацию о различных моделях в дальнейшем.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица модели\label{tbl:model}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*        | *Тип*    | *Описание*                           |
| Идентификатор   | Целое  | Уникальный идентификатор           |
| Название модели | Строка |                                    |
| Код типа модели | Целое  | Внешний ключ на таблицу тип модели |

Таблица текст сохраняет мета информацию о тексте отзыва.

#+ATTR_LATEX: :environment longtblr :options caption={Таблицы текст\label{tbl:text}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*                | *Тип*      | *Описание*                          |
| Идентификатор           | Целое    | Уникальный идентификатор          |
| Ссылка                  | Строка   | Ссылка на текст                   |
| Код источника           | Целое    | Внешний ключ из таблицы источники |
| Дата текста             | DateTime | Время публикации текста           |
| Заголовок               | Строка   | Заголовок текста                  |
| Код компании            | Целое    | Внешний ключ на компанию          |
| Количество комментариев | Целое    |                                   |

Так как Bert на вход принимает отдельные предложения, было решено сделать для них отдельную таблицу.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица предложений\label{tbl:sentence}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*          | *Тип*    | *Описание*                              |
| Идентификатор     | Целое  | Уникальный идентификатор              |
| Код текста        | Целое  | Внешний ключ из таблицы тексты        |
| Предложение       | Строка |                                       |
| Номер предложения | Целое  | Порядковый номер предложения в тексте |

Так как результат работы модели может отличать в зависимости от ее типа, то поле "результат" будет массивом.

#+ATTR_LATEX: :environment longtblr :options caption={Таблица результатов анализа текстов\label{tbl:text_result}} :align colspec={|X[l]|X[l]|X[l]|},rowhead = 1,hlines :position [h!]
| *Название*        | *Тип*                 | *Назначение*                                    |
| Идентификатор   | Целое               | Уникальный идентификатор                      |
| Код предложения | Целое               | Внешний ключ из таблицы предложения           |
| Код модели      | Целое               | Внешний ключ из таблицы модели                |
| Результат       | Вещественный массив | Результат работы модели                       |
| Обработано      | Логическое          | Показатель, обработано ли предложение или нет |

Диаграмма полученной схемы базы данных рис.\nbsp{}[[ref:fig:database]].
*** Проектирование базы данных для агрегации
При сборе функциональных требований было выявлено, что надо быстро показывать количество собранных отзывов и индекс компаний.

Обработанные данные из таблицы\nbsp{}\ref{tbl:text_result} агрегируются для каждого квартала и рассчитываются по формуле [[ref:eq:ethics]].
#+ATTR_LATEX: :environment longtblr :options caption={Таблица для расчета и показа индекса\label{tbl:index_calc}} :align colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines :position [h!]
| *Название*          | *Тип*          | *Описание*                                     |
| Идентификатор     | Целое        | Уникальный идентификатор                     |
| Год               | Целое        | Год за который был агрегирован индекс        |
| Квартал           | Целое        | Квартал за который был агрегирован индекс    |
| Название модели   | Строка       |                                              |
| Сайт источника    | Строка       |                                              |
| Тип источника     | Строка       |                                              |
| Название компании | Строка       |                                              |
| Код компании      | Целое        | Для запросов через API                       |
| Нейтральный       | Целое        | Количество нейтральных предложений за период |
| Позитивный        | Целое        | Количество позитивных предложений за период  |
| Негативный        | Целое        | Количество негативных предложений за период  |
| Базовый индекс    | Вещественное | Индекс для расчета итогового индекса         |
| Средний индекс    | Вещественное | Индекс для расчета итогового индекса         |
| Std индекс        | Вещественное | Индекс для расчета итогового индекса         |
| Индекс            | Вещественное | Рассчитанный индекс                          |

Собранные отзывы из таблицы\nbsp{}\ref{tbl:text} агрегируются для каждого месяца и рассчитывается количество собранных отзывов за месяц.
#+ATTR_LATEX: :environment longtblr :options caption={Таблица для расчета и показа индекса\label{tbl:index_calc}} :align colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines :position [h!]
| *Название*           | *Тип*      | *Описание*                                  |
| Идентификатор      | Целое    | Уникальный идентификатор                  |
| Дата               | DateTime |                                           |
| Квартал            | Целое    | Квартал за который был агрегирован индекс |
| Тип источника      | Строка   |                                           |
| Сайт               | Строка   |                                           |
| Количество отзывов | Целое    |                                           |

Диаграмма полученной схемы базы данных рис.\nbsp{}[[ref:fig:database_views]].
** Проектирование модуля работы с данными
Модуль будет представлять собой HTTP API для работой с базой данных.

Для работы с базой данных будут созданы классы, которые представляют ORM-модель для работы с базой данных.

При первом старте приложение будет получаться список компаний (банки, брокеры, микрокредитные организации и страховые) с сайта "Центрального банка России" и помещаться в базу данных. Из этих данных будет собираться лицензия компании и название компании, для микрокредитных организаций дополнительно будет собираться основной государственный регистрационный номер (ОГРН), так как под одной лицензией может работать несколько компаний. При последующих стартах приложение будет проверяться, что в каждом списке есть компании и новые компании не будут выгружаться.

Далее создаются объекты класса Bank с использованием полученных данных и добавляются в список @@latex: cbr\_banks@@, затем он возвращается как результат работы функции.

Таким образом, принцип работы данного алгоритма заключается в извлечении необходимых данных из HTML-кода веб-страницы и преобразовании их в объекты класса Bank, что позволяет автоматизировать процесс получения и анализа информации о компаниях. Диаграмма классов рис.\nbsp{}[[ref:fig:cbr_parser_class]].

#+begin_src d2 :exports results :file img/d2/cbr_parser_class.png
BaseParser: {
  shape: class

  create_bank_type(): BankType
  parse()
  get_bank_list()
  get_dataframe(url str, skip_rows int = 3, index_col str \| int \| None): "pd.DataFrame | None"
}

BankiParser: {
  shape: class

  create_bank_type(): BankType
  parse()
  get_bank_list(): "list[Bank]"
}

BrokerParser: {
  shape: class

  create_bank_type(): BankType
  parse()
  get_bank_list(): "list[Bank]"
}

InsuarenceParser: {
  shape: class

  create_bank_type(): BankType
  parse()
  get_bank_list(): "list[Bank]"
}

MfoParser: {
  shape: class

  create_bank_type(): BankType
  parse()
  get_bank_list(): "list[Bank]"
}

BaseParser -> BankiParser
BaseParser -> InsuarenceParser
BaseParser -> BrokerParser
BaseParser -> MfoParser
#+end_src

#+CAPTION: Диаграмма классов для сбора данных с сайта ЦБ
#+NAME: fig:cbr_parser_class
#+ATTR_LATEX: :placement [h!] :width \textwidth
#+RESULTS:
[[file:img/d2/cbr_parser_class.png]]

Для работы с источниками текстов необходимо сделать запросы для типов источников и самих источников. Также для обновления состояния сборщика данных надо сделать отдельный метод =PATCH=, который позволит обновлять время и состояние источника данных по идентификатору. Также при создании источника будет проверяться существует ли такой тип источника или нет. Если его не существует, то такой тип будет создаваться.

Сохранение текстов будет доступно по методу =POST= c передачей данных о тексте и состоянии сборщика данных. При выполнении запроса должно обновляться состояние сборщика данных, а каждый текст должен сохраняться, как набор предложений. При получении предложений должны выбираться такие предложения, которые еще не обработаны моделью.

Работа с моделями будет происходить аналогично источникам. При сохранении модели будет проверяться есть ли такой тип модели или нет. Если его нет, то он будет создан.

Также необходима возможность получения списка компаний с помощью API по различным сферам работы.

В результате проектирования должно получиться API, которое реализует запросы представленные в таблице @@latex:~\ref{tbl:api_doc}@@.
** Проектирование модуля агрегации данных
Для построения индекса этичности компаний будет ежедневно агрегироваться база данных. Результат обработки предложений будет группироваться для каждого отзыва по году, компании и типам источника. Из полученных данных будет строиться индексы согласно формуле описанной раньше [[ref:eq:ethics]].
** Проектирование модуля сбора данных
У всех сборщиков данных одинаковый принцип работы (рис.\nbsp{}[[ref:fig:parser_flow]]):
1. Сборщик данных запрашивает у модуля работы с базой данных список сохраненных компаний. Модуль отвечает на запрос, отправляя список сохраненных компаний обратно.
2. Сборщик данных запрашивает у сайта для сбора данных список компаний на сайте. Сайт отправляет список компаний обратно в сборщик данных.
3. После получения списка компаний, сборщик данных сохраняет только те компании, которые уже есть в основной базе данных. Это делается для того, чтобы связать компании которые представлены на сайте и в базе данных.
4. Затем, сборщик данных начинает собирать данные для каждой компании из списка. Это может быть сделано путем отправки запросов к API сайта или сканирования страниц сайта для поиска нужных данных. Собранные данные затем сохраняются в основной базе данных. Сбор данных будет происходить до тех пор пока не соберутся все отзывы для компании, или дата отзыва дойдет до даты предыдущего сбора данных.

Для реализации сборщиков данных было решено сделать базовый класс, который представляет собой интерфейс с функцией =parse=. Из него наследуются интерфейсы для сбора данных для каждого сайта (banki.ru, sravni.ru, vk.com). Диаграмма классов рис.\nbsp{}[[ref:fig:parser_class_diagram]]. От этих базовых классов для каждого сайта будут наследоваться классы, которые собирают отзывы компаний из различных сфер. Было выбрано такое решение, так как представление информации в рамках одного сайта в различных разделах может сильно различаться. Также у каждого сборщика данных будет своя база данных для сохранения информации о компаниях.

#+begin_src mermaid :exports results :file img/mermaid/parser_flow.png :exports none
%%{
   init: {
     "theme": 'base',
     "themeVariables": {
       "primaryColor": '#FFF',
       "primaryTextColor": '#000',
       "primaryBorderColor": '#000',
       "lineColor": '#FFF'
     }
   }
}%%

sequenceDiagram
    participant A as Сборщик данных
    participant B as Сайт для сбора данных
    participant API as Модуль работы с данными
    participant DB as База данных<br/>сборщика данных
    A->>API: Получить список<br/>сохраненных компаний
    API->>A: Список сохраненных компаний
    A->>B: Получить список компаний на сайте
    B->>A: Список компаний с сайта
    A->>DB: Сохранение компаний,<br/>которые есть на сайте и в основной БД
    A->>B: Отправка запроса для получения данных
    B->>A: Текст отзывов
    A->>API: Отправка полученных отзывов
#+end_src

#+CAPTION: Схема работы сборщиков данных
#+NAME: fig:parser_flow
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
[[file:img/mermaid/parser_flow_draw.png]]

*** Проектирование сбора данных с banki.ru
Для получения данных с сайта banki.ru будут отправляться запросы на их внутренний API. Для запросов надо иметь идентификатор компании с сайта, также надо иметь идентификатор компании из модуля работы с базой данных. Исходя из требований получилась база данных @@latex:~\ref{tbl:banki_ru}@@. Диаграмма полученной схемы базы данных рис.\nbsp{}[[ref:fig:database_banki_ru]].

#+ATTR_LATEX: :environment longtblr :options caption={Таблица для сайта banki.ru\label{tbl:banki_ru}} :align colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines :position [h!]
| *Название*               | *Тип*    | *Описание*                                   |
| Идентификатор          | Целое  | Уникальный идентификатор                   |
| Идентификатор компании | Целое  | Идентификатор банка в основной базе данных |
| Имя компании           | Строка |                                            |
| Код компании           | Строка | Код компании для запросов по API           |

С этого сайта будут собираться данные о компаниях из пяти сфер:
1. *Отзывы на банки.*
   Список банков будет получаться из [[https://www.banki.ru/widget/ajax/bank_list.json]]. Затем они будут сравниваться по номеру лицензии с банками, которые есть в базе данных. Для получения отзывов о банках будут отправляться запросы на [[https://www.banki.ru/services/responses/list/ajax/]] и в параметры ссылки будет передаваться код банка и номер страницы с отзывами и из полученного json будут собираться данные об отзывах.
2. *Новости о банках.*
   В качестве списка компаний будет использоваться такой же список, как и для банков. Для получения текста новостей сначала будет собираться список новостей для компании. Для этого будут отправляться запросы на [[https://www.banki.ru/banks/bank/{bank.bank_code}/news/]] в зависимости от банка. Затем по каждой ссылке будет обрабатываться html код страницы и собираться текст новости.
3. *Отзывы на страховые компании.*
   Список компаний будет получаться из [[https://www.banki.ru/insurance/companies/]]. Затем они будут сравниваться по номеру лицензии со страховыми, которые есть в базе данных. После этого будут собираться отзывы по [[https://www.banki.ru/insurance/companies/]]. Затем из каждой страницы компании для будет обрабатываться html код страницы и браться данные отзывов.
4. *Отзывы на брокеров.*
   Для получения списка компаний данные будут браться из [[https://www.banki.ru/investment/brokers/list/]]. Затем они будут сравниваться по номеру лицензии с брокерами, которые есть в базе данных. После этого будут собираться отзывы по [[https://www.banki.ru/investment/responses/company/broker/]]. Затем из каждой страницы компании для будет обрабатываться html код страницы и браться данные отзывов.
5. *Отзывы на микрокредитные организации.*
   Для получения списка компаний данные будут браться из [[https://www.banki.ru/microloans/ajax/search]]. Затем они будут сравниваться по номеру лицензии и ОГРН с компания, которые есть в базе данных. После этого будут собираться отзывы по [[https://www.banki.ru/microloans/responses/ajax/responses/]]. Затем из полученного json собираются отзывы о компании.
В конце сбора данных для каждого типа компаний собранные отзывы будут отправляться в модуль работы с базой данных.
*** Проектирование сбора данных с sravni.ru
Для получения данных с сайта sravni.ru будут отправляться запросы на их внутренний API. Для запросов надо иметь идентификатор компании с сайта, также надо иметь идентификатор компании из модуля работы с базой данных, также для некоторых запросов надо иметь псевдоним компании (alias). Исходя из требований получилась база данных@@latex:~\ref{tbl:sravni_ru}@@. Диаграмма полученной схемы базы данных рис.\nbsp{}[[ref:fig:database_sravni_ru]].

#+ATTR_LATEX: :environment longtblr :options caption={Таблица для сайта sravni.ru\label{tbl:sravni_ru}} :align colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines :position [h!]
| *Название*                     | *Тип*    | *Описание*                                   |
| Идентификатор                | Целое  | Уникальный идентификатор                   |
| Идентификатор компании       | Целое  | Идентификатор компании в основной базе данных |
| Код банка в sravni.ru        | Целое  |                                            |
| Старый код компании в sravni.ru | Целое  |                                            |
| Псевдоним компании           | Строка |                                            |
| Название компании       | Строка |                                            |
Диаграмма полученной схемы базы данных рис.\nbsp{}[[ref:fig:database_sravni_ru]]

С этого сайта будут собираться данные о компаниях из трех сфер:
1. *Отзывы на банки.*
   Список банков будет получаться из [[https://www.sravni.ru/proxy-organizations/organizations]] с параметром =organizationType= равным =bank=. Затем они будут сравниваться по номеру лицензии с банками, которые есть в базе данных. Для получения отзывов о банках будут отправляться запросы на [[https://www.sravni.ru/bank/{bank_info.alias}/otzyvy/]] и в параметры ссылки будет передаваться псевдоним банка и номер страницы с отзывами. И из полученного json будут собираться данные об отзывах.
2. *Отзывы на страховые компании.*
   Список банков будет получаться из [[https://www.sravni.ru/proxy-organizations/organizations]] с параметром =organizationType= равным =insuranceCompany=. Затем они будут сравниваться по номеру лицензии со страховыми, которые есть в базе данных. Для получения отзывов о банках будут отправляться запросы на [[https://www.sravni.ru/strakhovaja-kompanija/{bank_info.alias}/otzyvy/]] и в параметры ссылки будет передаваться псевдоним страховой и номер страницы с отзывами. И из полученного json будут собираться данные об отзывах.
3. *Отзывы на микрокредитные организации.*
   Список банков будет получаться из [[https://www.sravni.ru/proxy-organizations/organizations]] с параметром =organizationType= равным =mfo=. Затем они будут сравниваться по номеру лицензии и ОГРН с компаниями, которые есть в базе данных. Для получения отзывов о банках будут отправляться запросы на [[https://www.sravni.ru/zaimy/{bank_info.alias}/otzyvy/]] и в параметры ссылки будет передаваться псевдоним банка и номер страницы с отзывами. И из полученного json будут собираться данные об отзывах.
В конце сбора данных для каждого типа компаний собранные отзывы будут отправляться в модуль работы с базой данных.
*** Проектирование сбора данных с vk.com
Для получения на сайт vk.com будут отправляться запросы на их API. Для этого предварительно будут собраны данные о всех организациях, которые у них представлены на сайте и перемещены в базу данных \ref{tbl:vk_com}. Диаграмма полученной схемы базы данных рис.\nbsp{}[[ref:fig:database_vk_com]].
#+ATTR_LATEX: :environment longtblr :options caption={Таблица для сайта vk.com\label{tbl:vk_com}} :align colspec={|X[2,l]|X[1,l]|X[3,l]|},rowhead = 1,hlines :position [h!]
| *Название*                 | *Тип*    | *Описание*                 |
| Идентификатор            | Целое  | Уникальный идентификатор |
| Идентификатор на vk.com  | Строка |                          |
| Имя компании             | Строка |                          |
| Домен компании на vk.com | Строка |                          |

Для доступа к API будет зарегистрировано приложение для получения ключа к нему. Для каждой компании будут выгружаться посты пока дата последней выгрузки не более чем дата последнего поста для этого будет отправляться запрос на [[https://api.vk.com/method/wall.get]], куда будет подставляться токен приложения и идентификатор группы. Затем для каждого поста будут выгружаться комментарии по методу [[https://api.vk.com/method/wall.getComments]], а затем отправляться в модуль работы с базой данных.
** Проектирование модуля обработки данных
Данный модуль будет обрабатывать полученные отзывы с помощью полученной модели. Для этого каждый день он будет запускаться, получать тексты из модуля работы с базой данных и отправлять из обратно.
** Выводы по главе
В данной главе были представлены результаты проектирования системы и ее отдельных модулей и их взаимодействие, включая базы данных и API, согласно выявленным требованиям из первой главы. Каждый модуль был спроектирован с учетом принципов микросервисной архитектуры и обеспечивает функциональность, необходимую для реализации системы в целом.

Была спроектирована база данных для хранения информации об отзывах, источниках, моделях и компаниях. Она была спроектированы с учетом требований к масштабируемости и производительности системы.

Было проведено проектирование методов для лучшего подбора метода обработки текста и модели классификации. Для этого была подобрана метрика, которая позволит лучшим образом отразить результат подбора моделей.

Эти результаты будут использоваться при разработке и реализации системы в следующих этапах проекта.
* Реализация системы
В данной главе описывается реализация системы и каждого модуля, обучение модели.

Этап реализации можно разделить на пункты:
1. Реализация базы данных.
2. Реализации модулей для собора, работы и агрегации данных.
3. Обучение модели и реализация модуля обработки данных.
4. Развертывание системы.
** Реализация определения наиболее подходящей модели
*** Обучение алгоритмов обработки естественного языка
Перед обучением алгоритмов надо обработать текст, чтобы повысить их эффективность. В первую очередь была осуществлена очистка от распространенных слов, также известных как стоп-слова, и были удалены знаки пунктуации.

Для лемматизации слов была применена библиотека spaCy[cite:@boyd_explosion_2023], которая предоставила доступ к модели для русского языка "@@latex:ru\_core\_news\_md@@". Лемматизация является важным этапом, поскольку она помогает уменьшить размерность векторного пространства, используемого в методах анализа текстов, таких как мешок слов и TF-IDF.

Затем был произведен подсчет количество вхождений каждого слова (см. рисунок [[ref:fig:word_freq_total]]). Дополнительно, из текстов были удалены слова, которые встретились менее 10 раз, так как они вносят излишнюю сложность в векторное пространство. В общей сложности такие слова составляют 85% от общего числа слов (см. рисунок [[ref:fig:word_freq_low]]).

#+CAPTION: График распространенности слов
#+NAME: fig:word_freq_total
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
[[file:img/word_freq_total.png]]

#+CAPTION: График распространенности слов, которые встретились меньше 100 раз
#+NAME: fig:word_freq_low
#+ATTR_LATEX: :placement [h] :width 0.8\textwidth
[[file:img/word_freq_low.png]]

В результате работы алгоритма из текста "Тем самым оставив меня без средств к существованию, тем более я многодетный отец, единственный кормилец семьи!!!" получится "самым оставить средство единственный семья". Такие тексты подавались на вход мешка слов, TF-IDF и FastText.

Для обработки текста с использованием модели Word2Vec была выбрана модель "word2vec-ruscorpora-300"[cite:@kutuzov_webvectors_2017] из библиотеки gensim[cite:@rehurek_software_2010]. Однако, для корректной работы этой модели, необходимо добавить часть речи к каждому слову. Например, для слова "человек" необходимо добавить метку "NOUN", так как оно является существительным. Таким образом, входными данными для модели должно быть "человек_NOUN". Однако, такое преобразование текста усложняет использование данной модели, так как требуется дополнительная модель для определения частей речи слова. В связи с этим, было принято решение дополнительно воспользоватсья реализацией Word2Vec из библиотеки Navec.

Для обработки текста с помощью BERT будет использоваться 2 модели:
1. Модель от =DeepPavlov= =rubert-base-cased=\nbsp{}[cite:@kuratov_adaptation_2019], она обучена на русской Википедии и данных из Вконтаке
2. Модель от =blanchefort= =rubert-base-cased-sentiment-rurewiews=, за основу этой модели взят =rubert-base-cased= и был дообучен на данных отзывов, твиттера и отзывах о медицинских учреждениях.

В обе модели будет подаваться как исходный, необработанный текст, так и обработанный текст. Исходный текст используется, поскольку данные модели обучены на большом объеме текста и способны учитывать контекст каждого слова внутри предложения, что делает их более устойчивыми и позволяет достичь более точных результатов.

Кроме того, для оценки качества работы модели будет проведено дообучение модели RuBERT на специально собранных данных. Дообучение будет выполняться путем решения задачи классификации предложений. Для этого будет добавлен слой нейронов к предобученной модели, принимающий векторные представления исходного текста и предсказывающий вероятности принадлежности к различным классам. Этот слой называется "головой классификации" (classification head).

Модель была дообучена в течение 7 эпох на собранных данных, при этом последовательно размораживались слои модели. Для лучшего качества обучения модели использовался оптимизатор =Adam=\nbsp{}[cite:@kingma_adam_2017]. В результате дообучения были достигнуты следующие значения метрики F1 0.75 при дообучении последнего слоя (см. рисунок [[ref:fig:bert_unfreeze_last]]) и 0.77 при дообучении последних двух слоев (см. рисунок [[ref:fig:bert_unfreeze_two_last]]). Эти результаты свидетельствуют о достигнутом качестве модели в решении задачи классификации текстов.

#+CAPTION: График изменения метрики F1 при обучении последнего слоя
#+NAME: fig:bert_unfreeze_last
#+ATTR_LATEX: :placement [h] :width 0.8\textwidth
[[file:img/DeepPavlovrubert-base-cased F1 Unfreeze last layer.png]]

#+CAPTION: График изменения метрики F1 при обучении последних 2 слоев
#+NAME: fig:bert_unfreeze_two_last
#+ATTR_LATEX: :placement [h] :width 0.8\textwidth
[[file:img/DeepPavlovrubert-base-cased F1 Unfreeze last 2 layers.png]]

Для получения эмбеддингов из моделей BERT в цикле в модель подавались предложения и брались данные из последнего слоя и сохранялись.
*** Обучение алгоритмов классификации
Для лучшего сохранения результатов использовалась библиотека MLFlow[cite:@chen_developments_2020]. Она поможет лучше сохранять и следить за результатами обработки. Для подбора гиперпараметров будет использоваться библиотека Optuna[cite:@akiba_optuna_2019].

Для удобной интеграции с MLFlow цикл обучения каждой модели помещался в отдельную папку. Тип данных, который принимает на вход модель получается из аргументов с которым она запущена. Затем для каждой модели запускается 30 итераций для подбора гиперпараметров с помощью optuna. Для каждой итерации в MLFlow сохраняется параметры с которыми обучалась модель и результат метрики F1. После обучения всех моделей для подбора гиперпараметров выбираются наиболее оптимальные параметры и обучается итоговая модель и сохраняется в MLFlow. В итоге каждая модель запускалась с каждым типом обработки текста.
*** Результаты
В результате получилось, что лучший результат дало дообучение RuBERT и логистическая регрессия (см. рисунок [[ref:fig:training_res]]).

#+CAPTION: Результат обучения моделей
#+NAME: fig:training_res
#+ATTR_LATEX: :placement [h] :width 0.8\textwidth
[[file:img/runs.png]]

#+ATTR_LATEX: :environment longtblr :options caption={Таблица результатов обучения моделей\label{tbl:model_data_compare}} :align colspec={|X[2,l]|X[r]|X[r]|X[r]|X[r]|X[r]|X[r]|},rowhead = 1,hlines :position [h]
| *Тип входных данных*                  | *logreg* |  *svm* | *forest* | *gradient* | *xgboost* | *catboost* |
| Мешок слов                          |   0.69 | 0.68 |   0.44 |     0.69 |    0.69 |     0.69 |
| TF-IDF                              |   0.69 | 0.66 |   0.44 |     0.65 |    0.66 |     0.66 |
| Word2Vec                            |   0.66 | 0.68 |   0.53 |     0.65 |    0.66 |     0.67 |
| Word2Vec Navec                      |   0.69 | 0.71 |   0.54 |     0.68 |    0.68 |     0.69 |
| fastText                            |   0.44 |  0.5 |   0.43 |     0.48 |    0.49 |     0.51 |
| RuBERT базовый                      |   0.75 | 0.73 |   0.54 |     0.68 |    0.67 |     0.69 |
| RuBERT базовый обработанные тексты  |   0.63 | 0.63 |    0.5 |     0.61 |    0.62 |     0.63 |
| RuBERT эмоции                       |   0.79 | 0.79 |   0.69 |     0.76 |    0.76 |     0.78 |
| RuBERT эмоции обработанные тексты   |   0.68 | 0.68 |   0.57 |     0.65 |    0.66 |     0.66 |
| RuBERT разморожен 1 последних слоя  |   0.77 | 0.78 |   0.72 |     0.76 |    0.77 |     0.77 |
| RuBERT разморожены 2 последних слоя |   0.83 | 0.83 |    0.8 |     0.81 |    0.82 |     0.82 |
** Реализация базы данных
Для хранения информации в системе была выбрана СУБД PostgreSQL. Для создания базы данных был выбран подход "code first", который позволяет определить структуру базы данных в виде классов на языке Python. Для этого использовалась библиотека Sqlalchemy\nbsp{}[cite:@bayermichael_architecture_2012], которая обеспечивает ORM-модель для работы с базами данных. При запуске приложения база данных будет создаваться автоматически на основе определенных классов.

Для определения структуры базы данных был создан базовый класс =DeclarativeBase=, который является родительским для всех классов, определяющих таблицы базы данных. Каждая таблица базы данных определяется в виде отдельного класса, который наследует базовый класс и содержит определения столбцов и связей между таблицами.

Для ускорения работы запросов все поля, которые являются внешними ключами были проиндексированы. Также в таблице с информацией о текстах были добавлены индексы, которые извлекают из даты год и квартал.

Для обеспечения возможности модернизации базы данных в дальнейшем была использована библиотека alembic, которая обеспечивает миграции базы данных и позволяет вносить изменения в структуру базы данных без потери данных.
** Реализация модуля работы с базой данных
Для реализации API используется асинхронный фреймворк FastAPI и для взаимодействии с базой данных асинхронная библиотека asyncpg. Для валидации приходящих данных и ответов для каждого запроса была создана своя модель с помощью библиотеки Pydantic. Также с помощью Pydantic был сделан класс для получения строки подключения к базе данных из переменных окружения.

При старте приложения сначала проверятся подключение с базой данных и проверяется ее версия, если она не актуальна, то выполняются миграции для ее актуализации. Затем проверятся список компаний, если список компаний пустой, то собирается данные о банках, брокера, страховых и микрофинансовых организациях.

Информация о банках будет собираться по ссылке [[https://www.cbr.ru/banking_sector/credit/FullCoList/]]. Алгоритм начинается с получения объекта BeautifulSoup\nbsp{}[cite:@richardsonleonard_beautiful_2007], который содержит HTML-код веб-страницы. Затем происходит итерация по всем элементам таблицы, начиная со второй строки, так как в первой находится заголовки для каждой колонки. Для каждой строки таблицы находятся все ячейки, извлекаются регистрационный номер (номер лицензии) и название банка. В списке также есть платежные небанковские кредитные организации, которые имеют буквы на конце лицензии, например =3511-К= у "Деньги.Мэйл.Ру". Для этого такие номера будут разделяться по "-" и браться номер и преобразовываться в число. Затем собранные данные помещаются в базу данных.

Для сбора данных о брокерах будет обрабатываться excel файл, который доступен по ссылке [[https://www.cbr.ru/vfs/finmarkets/files/supervision/list_brokers.xlsx]], с помощью библиотеки pandas\nbsp{}[cite:@team_pandasdev_2023]. При запуске происходит загрузка таблицы с данными о брокерах в формате Excel, после чего данные из таблицы считываются. Затем происходит итерация по строкам таблицы и для каждой строки создается экземпляр класса Bank, который содержит информацию о банке-брокере, такую как номер лицензии, наименование организации и тип банка. Для удобства хранения номера лицензии, из них удалялись все знаки "-".

Для сбора данных о страховых будет обрабатываться excel файл, который доступен по ссылке [[https://www.cbr.ru/vfs/finmarkets/files/supervision/list_ssd.xlsx]]. Так как в файле много строк, которые не содержат номеров или наименований банков, то они удаляются из него. Номера лицензий хранятся в формате =СИ № 3847= или =ОС № 1083 - 05= и для получения номера берется первое число которое встретилось в строке с помощью регулярного выражения. Затем полученная информация помещается в базе данных.

Для сбора данных о микрофинансовых организациях будет обрабатываться excel файл, который доступен по ссылке [[https://www.cbr.ru/vfs/finmarkets/files/supervision/list_ssd.xlsx]]. В этом файле номер лицензии разбит по 5 ячейкам и в части из отсутствуют числа. Поэтому отсутствующие ячейки заполняются нулями и содержание ячеек объединяется для получения результата. Потом также берется название компании и эта информация помещается в базу данных.

API было реализовано согласно требованиям описанными во второй главе.

Алгоритм получения предложений для обработки проверяет, какие из них уже были обработаны моделью, а какие - нет. Если для каждого запроса искать пересечение множества предложений, которые еще не обработаны моделью и уже обработаны, это может занять много времени. Поэтому сначала выполняется запрос\nbsp{}([[ref:lst:insert_unused]]), который ищет предложения, еще не обработанные моделью. Если таких нет, то в таблицу с результатами добавляются 100 000 предложений с пустыми результатами, чтобы было проще искать предложения при дальнейших запросах. Затем с помощью запроса\nbsp{}([[ref:lst:select_unused]]) из таблицы с результатами выбираются предложения, еще не обработанные моделью. Ниже приведены SQL запросы, которые генерирует ORM.

#+NAME: lst:insert_unused
#+CAPTION: SQL запрос на вставку не обработанных предложений
#+begin_src sql
INSERT INTO text_result (text_sentence_id, model_id, is_processed)
SELECT text_sentence.id, :model_id, false
FROM text_sentence
JOIN text ON text_result.text_id = text.id
JOIN source ON text.source_id = source.id
LEFT JOIN (
  SELECT text_result.text_sentence_id
  FROM text_result
  WHERE text_result.model_id = :model_id
) AS subq ON text_sentence.id = subq.text_sentence_id
WHERE source.site IN (:sources) AND subq.text_sentence_id IS NULL
LIMIT 100000;
#+end_src

#+NAME: lst:select_unused
#+CAPTION: SQL запрос на получение еще не обработанных предложений
#+begin_src sql
SELECT text_sentence.id, text_sentence.sentence
FROM text_sentence
JOIN (
  SELECT text_result.text_sentence_id, text_result.id
  FROM text_result
  WHERE text_result.model_id = :model_id AND text_result.is_processed = false
  LIMIT :limit
) AS sub
ON text_sentence.id = sub.text_sentence_id;
#+end_src

Для разделение текста на предложения при получении текста используется библиотека =nltk=\nbsp{}[cite:@bird_natural_2009].

Для валидации параметров отвечающих за тип индекса этичности, список источников и период агрегации для получения агрегированных данных были сделаны =Enum=-классы. Если в запрос для получения статистики был передан параметр показывающий, что надо агрегировать только по годам, то в запрос подставлялась дополнительная часть с =group by=.

Для получения данных об обработанных предложения в зависимости от типа запрашиваемого индекса в запрос подставлялся нужный тип индекса и проводилась агрегация данных аналогично запросу на получение статистики.
** Реализация модуля агрегации данных
Для реализации этого модуля для взаимодействии с базой данных используется синхронная библиотека psycopg2, а в качестве ORM Sqlalchemy, для регулярного обновления данных используется библиотека schedule, которая позволяет делать регулярные операции.

При запуске модуля начинается подсчет количества собранных отзывов и расчет индекса этичности в разных потоках.

Так как в базе данных находится очень много элементов, то было решено обновлять данные напрямую из SQL. Код запроса на расчет статистки\nbsp{}[[ref:lst:count_reviews]].

#+NAME: lst:count_reviews
#+CAPTION: SQL запрос на подсчет количества предложений
#+begin_src sql
INSERT INTO text_sentence_count (count_reviews, date, quarter, source_type, source_site)
SELECT COUNT(text.id) AS reviews_count,
       DATE_TRUNC('month', text.date) AS month,
       EXTRACT('quarter' FROM text.date) AS quarter,
       source_type.name AS source_type,
       source.site AS source_site
FROM text
JOIN source ON text.source_id = source.id
JOIN source_type ON source.source_type_id = source_type.id
GROUP BY month, quarter, source.site, source_type.name;
#+end_src

Запрос для создания запроса\nbsp{}[[ref:lst:sql_aggregate]] на расчет данных было решено использовать несколько подзапросов:
1. Сначала рассчитывается логарифм результата обработки предложений для каждой колонки. Для избежания проблем с логарифмами к каждому значению добавляется маленькое число, так как у некоторые значения могут быть нулевыми. Этот подзапрос создан для того, чтобы ускорить выполнение, так как этот расчет можно было объединить со следующим подзапросом, но из-за этого пришлось бы пересчитывать одинаковые значения несколько раз.
2. Затем для подсчета предложений разных типов определяется их категория. Для этого используется конструкция =case when=, где значение обработанных категорий сравнивается попарно.
3. Потом к полученным данным присоединяются данные из других таблиц. Извлекается информация о квартале и дате, значения с предыдущего шага суммируются. Сам запрос объединяется для каждого квартала компаний, для каждого источника отдельно.
4. И в конце полученные данные вставляются в таблицу.
5. Затем уже на агрегированных данных рассчитываются значение индекса согласно формуле\nbsp{}[[ref:eq:ethics]].

#+name: lst:sql_aggregate
#+caption: SQL запрос на агрегацию обработанных предложений
#+begin_src sql
INSERT INTO aggregate_table_model_result (bank_id, bank_name, quater, year, model_name, source_site, source_type, positive, neutral, negative, total)
SELECT
    extract(year from text.date) as year,
    extract(QUARTER from text.date) as quarter,
    bank.id as "bank_id",
    model.name as "model_name",
    source.site as "source_site",
    source_type.name as "source_type_name",
    sum(positive) as "positive",
    sum(neutral) as "neutral",
    sum(negative) as "negative",
    sum(positive+neutral+negative) as total
FROM
    (SELECT
        text_sentence_id,
        model_id,
        case when (log_positive > log_neutral) and (log_positive > log_negative) then 1 else 0 end as "positive",
        case when (log_neutral  > log_positive) and (log_neutral > log_negative) then 1 else 0 end as "neutral",
        case when (log_negative > log_neutral) and (log_negative > log_positive) then 1 else 0 end as "negative"
    FROM (
        SELECT
            text_sentence_id,
            model_id,
            (LOG(result[1]+0.0000001)) as "log_neutral",
            (LOG(result[2]+0.0000001)) as "log_positive",
            (LOG(result[3]+0.0000001)) as "log_negative"
        FROM text_result
        WHERE model_id = 1) t) pos_neut_neg
JOIN
    text_sentence ON pos_neut_neg.text_sentence_id = text_sentence.id
JOIN
    text ON text_sentence.text_id = text.id
JOIN
    bank ON text.bank_id = bank.id
JOIN
    source ON source.id = text.source_id
JOIN
    source_type ON source.source_type_id = source_type.id
JOIN
    model ON model.id = pos_neut_neg.model_ida
GROUP BY quarter, year, source.site, source_type.name, bank.id, model.name
#+end_src
** Реализация модуля сбора данных

Для каждого сайта будет создана отдельная папка (модуль) со схожей структурой:

Для реализации этого модуля для взаимодействии с базой данных используется синхронная библиотека psycopg2, а в качестве ORM Sqlalchemy, для регулярного обновления данных используется библиотека schedule, которая позволяет делать регулярные операции, для обработки html страниц используется библиотека BeautifulSoup, также для обработки данных используется библиотека Pydantic.
1. В файле =database= будет лежать схема модели базы данных.
2. =schemes= pydantic модели для обработки текста.
3. =queries= запросы в базу данных.

Также для всех сборщиков данных была выделена общая часть, включающая модуль запросов, модулей объектов и настроек, а также модуль для запросов к базе данных. Модуль запросов является модификацией библиотеки requests\nbsp{}[cite:@chandra_python_2015] и предоставляет возможность повторного выполнения запросов в случае неудачи и обработки формата json. Модуль моделей содержит pydantic классы объектов для работы с запросами к базе данных и обработки данных. Модуль настроек представляет pydantic класс, который получает данные о подключении к базе данных, ссылке на API и токен для работы с API ВКонтаке из окружения приложения. Модуль для запросов к API предоставляет набор функций для выполнения запросов.

Для удобства развертывания было решено запускать сборщик данных в зависимости от аргумента с которым запущен код. Потом при запуске в зависимости от переданных аргументов создается база данных и запускается сборщик. Процесс сбора данных запускается ежедневно с помощью библиотеки schedule.

*** Разработка модуля сбора данных с banki.ru
Сбор данных с bani.ru осложнен тем, что компании из разных сфер имеют разное представление на сайте, поэтому для каждой сферы нужен свой подход. Также стоит отметить, что для успешной отправки запросов на сайт, требуется в заголовках запроса добавлять параметр "X-Requested-With" со значением "XMLHttpRequest".

Для сбора данных был создан базовый класс, который реализует главный цикл сбора дынных. При запуске сборщика данных проверяется загружен ли список компаний в базу данных или нет, если нет то в базу данных загружается список компаний с сайта и проверяется какие компании уже есть в основной базе данных. Затем полученные компании сохраняются в базе данных сборщика. Для этого каждый класс должен будет реализовать функцию для получения списка компаний =load_bank_list=. Затем запускается сбор данных. Сначала получается на каком момента остановился сборщик данных в прошлый раз из модуля по работе с базой данных. Далее берется количество страниц отзывов у компании. Потом для каждой компании берутся тексты с помощью функции =get_page_bank_reviews= и сохраняются тексты, которые не были еще собраны. Затем полученные тексты в модуль работы с базой данных.

Как реализована функция =load_bank_list= для различных сфер:
1. *Банки*. Для получения этого списка компаний будет отправляться запрос по адресу [[https://www.banki.ru/widget/ajax/bank_list.json]] и из полученного json собираться список компаний.
2. *Страховые*. Для получения списка компаний сначала загружается html страница со списком по адресу [[https://www.banki.ru/insurance/companies/]]. Затем в ищется элемент div с атрибутом =data-module= равным =ui.pagination=. Из этого элемента из атрибута =data-options= получается количество компаний и страниц с ними. Потом для каждой страницы с компаниями ищутся все элементы =tr= с атрибутом =data-test= равным =list-row=. Из этого элемента получается вся информация о компании. Потом полученные компании сравниваются с теми, что сохранены в основной базе данных и сохраняются в базу сборщика данных.
3. *Брокеры*. Для получения списка компаний отправляется запрос по адресу [[https://www.banki.ru/investment/brokers/list/]], но без дополнительного заголовка, так как только без него появляются лицензии компаний. Потом из этого списка собирается информация о компаниях и сохраняется в базе данных.
4. *Микрофинансовые организации*. Для получения списка компаний отправляется запрос по адресу [[https://www.banki.ru/microloans/ajax/search]]. Сначала из полученного json получается количество страниц с компаниями. Затем для каждой страницы отправляются новые запросы и обрабатывается информация о компаниях. Потом собранные компания сравниваются с компаниями из основной базы данных по номеру лицензии и ОГРН с компаниями из основной базы данных и сохраняются в базе данных сборщика данных.

Реализация функции =get_page_bank_reviews= для различных сфер:
1. *Банки*. Для получения отзывов будет делаться запрос по адресу [[https://www.banki.ru/services/responses/list/ajax/]] с параметрами для определения компании и номера страницы. Из полученного json соберутся отзывы и отправятся в основную базу данных.
2. *Новости*. Для получения текстов новостей сначала будут собираться адреса новостей, а затем уже сами тексты новостей. Для сбора адресов будет отправляться запрос на [[https://www.banki.ru/banks/bank/{bank_code}/news/]], где "=bank_code=" код банка, также в качестве параметра запроса будет отправляться номер страницы. Для получения адресов будут браться элементы "a" с классом "text-list-link", также для отбора новых новостей будут обрабатываться даты. Для этого будут браться элементы "span" с классом "text-list-date". Потом по полученным ссылкам будет браться html код страниц и браться текст новости из элементов "p".
4. *Страховые и Брокеры*. Для этих сфер тексты отзывов получаются путем обработки html страниц. Они получаются из запросов на адреса [[https://www.banki.ru/investment/responses/company/broker/]] для брокеров и [[https://www.banki.ru/insurance/responses/company/]] для страховых, к этим ссылкам добавляется код компании и номер страницы для получения отзывов. Потом для получения текста отзывов ищутся элементы "div" с классом "=responses__item__message=" и из него берется текст. Затем собранные отзывы отправляются в модуль работой с базой данных.
6. *Микрофинансовые организации*. Для получения отзывов отправляются запросы на [[https://www.banki.ru/microloans/responses/ajax/responses]], где в параметры передаются код компании и номер страницы. Затем из полученного json собираются отзывы и отправляются в основную базу данных.

*** Разработка модуля сбора данных с sravni.ru
При сборе данных со sravni.ru будут отправляться запросы на их внутреннее API, которое имеет схожую структуру для всех сфер компаний. При запуске сборщика данных проверяется загружен ли список компаний в базу данных или нет, если нет то в базу данных загружается список компаний. Он будет получать путем отправки запроса на [[https://www.sravni.ru/proxy-organizations/organizations]] с различным значением параметра =organizationType= ("bank" для банков, "insuranceCompany" для страховых компаний и "microcredits" для микрофинансовых организаций). Потом полученный список компаний проверяется со списком, который сохранен в основной базе данных. Затем полученные компании сохраняются в базе данных сборщика.

Затем запускается процесс сбора данных. Сначала получается на каком момента остановился сборщик данных в прошлый раз из модуля по работе с базой данных. Потом для каждой компании получается список отзывов. Он получается путем отправки запроса по адресу [[https://www.sravni.ru/proxy-reviews/reviews]] с параметром "reviewObjectType" с такими же значениями, как для получения списка компаний, и идентификатором компании на сайте sravni.ru. В результате запроса получается json, в котором находится 1000 отзывов на компанию. Из этих отзывов выбираются новые отзывы с момента предыдущего сбора данных. Потом собранные данные отправляются в основную базу данных.
*** Разработка модуля сбора данных с vk.com
Для взаимодействия с API ВКонтакте был реализован класс, который делает запросы к API и подставляет обязательные параметры, такие как токен и версия API, так и параметры которые нужны для различных методов. Также этот класс регулирует количество запросов к API, так как разрешено делать не более трех запросов в секунду.

При запуске сборщика данных проверяется загружен ли список компаний в базу данных или нет, если нет то в базу данных загружается список отобранных заранее компаний. Затем запускается процесс сбора данных. Сначала получается на каком момента остановился сборщик данных в прошлый раз из модуля по работе с базой данных. Затем для каждой компании берет публикации в группе. Для публикаций у которых разница во времени с момента предыдущего сбора данных не более недели собираются новые комментарии. Из собранных комментариев удаляются эмоджи и идентификаторы пользователей из ссылок на профиля ВКонтакте, которые имеют вид =(ID пользователя|Имя пользователя)=. Потом собранные комментарии отправляются в модуль работы с базой данных.
** Реализация модуля обработки текста
При запуске данного модуля происходит загрузка модели, которая будет использоваться для обработки текстов. После загрузки модель готова принимать новые тексты из модуля работы с базой данных. Эти тексты могут быть получены, например, из внешних источников или из предыдущих этапов обработки данных.

После получения текстов модель выполняет их обработку, применяя соответствующие алгоритмы и методы анализа. Обработанные данные отправляются обратно в модуль работы с базой данных. Таким образом, информация, полученная в результате обработки текстов моделью, становится доступной для дальнейшего использования или анализа в рамках системы.

Этот процесс запускается ежедневно, что позволяет обрабатывать новые тексты, поступающие в систему с течением времени. Регулярное выполнение данного процесса обеспечивает актуальность и своевременность обработки текстов, а также поддерживает работоспособность системы в целом.
** Развертывание системы
Для развертывания системы каждый компонент был выделен в отдельный контейнер docker\nbsp{}[cite:@merkel_docker_2014], а для оркестрации приложений существует инструмент Docker Compose, который позволяет запускать многоконтейнерные приложения с помощью YAML-файлов конфигурации. Для установки переменных окружения в контейнеры используется файл ".env", в котором содержались переменные окружения для всех приложений и путь к этому файлу прописывался в параметрах =env_file=. Для установки в названия базы данных дополнительно в конфигурации контейнеров указывалась переменная окружения =POSTGRES_DB=.

Для реализации модуля взаимодействия с базой данной в качестве базового образа использовался =python:3.10=. Потом устанавливалась библиотека nltk и данные для работы этой библиотеки с русским языком. Затем устанавливались зависимости приложения и оно запускалось с помощью библиотеки uvicorn. Для модулей агрегации базы и сбора данных в качестве базового образа использовался =python:3.10-slim=, так как он использует меньше памяти, чем обычный.

В данной работе определен сервис "database", который запускает контейнер с PostgreSQL версии 14.4. Контейнеру также присваиваются тома для хранения данных, которые будут использоваться внутри контейнера. Для обеспечения доступности сервиса в контейнере определены порты, через которые можно подключаться к базе данных. Кроме того, в конфигурации определен healthcheck, который проверяет работоспособность сервиса, выполняя команду =pg_isready= с заданными параметрами. Этот healthcheck запускается каждые 10 секунд и проходит 5 попыток, если проверка не прошла в заданный таймаут в 5 секунд. Такой подход обеспечивает более стабильную работу контейнера и позволяет оперативно реагировать на возможные проблемы.

Для модуля работы с базой данных определен сервис "api", который зависит от сервиса базы данных и начнет работу только после того, как у базы данных пройдет healthcheck.

Для модуля сбора для каждого сайта создавался новый контейнер и они запускались с помощью команды =python main.py --site name=, где вместо name было название сайта и из какой сферы собирались тексты. Все сборщики данных были выделены в отдельный профиль для удобства запуска.
** Выводы по главе
В данной главе представлена реализация системы и ее отдельных модулей, включая базы данных и API, согласно выявленным требованиям из первой главы. Каждый модуль был разработан с учетом принципов микросервисной архитектуры и обеспечивает определенную функциональность, необходимую для реализации системы в целом, согласно проектированию описанном в предыдущей главе. Был описан процесс сбора данных о различных компаниях с разных сайтов. Также был описан способ развертывания системы с помощью docker-compose.

Для анализа текстов было выбрано сочетание моделей дообученного BERT с размороженными двумя слоями и логистической регрессии, что дало результат метрики F1 0.83.

В результате было собрано (рис.\nbsp{}[[ref:fig:collected_data]]) 10 миллионов предложений или 2,5 миллиона отзывов для разных компаний. Они были обработаны с помощью разработанной модели (рис.\nbsp{}[[ref:fig:ethics_analisys]]) и были сохранены в базе данных.

#+CAPTION: График собранных предложений
#+NAME: fig:collected_data
#+ATTR_LATEX: :placement [h!] :width 0.6\textwidth
[[file:img/reviews_count.png]]

#+CAPTION: График оценки этичности компаний
#+NAME: fig:ethics_analisys
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
[[file:img/ethics_plot_index_safe.png]]
* Тестирование системы
В данной главе описывается разные методы тестирования, которые были использованы при реализации системы и форматтеры кода и инструменты статического анализа также является важным, поскольку это позволяет снизить количество ошибок и повысить качество кода.
** Форматирование кода
Во время реализации системы использовались различные инструменты для форматирования кода и обеспечения его качества, такие как black, isort, pyupgrade, flake8 и mypy.

Black — это инструмент для автоматического форматирования кода на языке программирования Python. Он помогает унифицировать стиль кодирования и повысить читаемость программного кода. Black применяет строгий набор правил форматирования, основанный на принципах PEP 8, и автоматически применяет эти правила к коду. Таким образом, он обеспечивает согласованность форматирования в проекте и упрощает совместную работу между разработчиками.

Isort — это инструмент для автоматической сортировки импортов в коде на языке Python. Он обеспечивает единообразное расположение импортов, что способствует повышению читаемости кода и уменьшению вероятности возникновения конфликтов и ошибок, связанных с порядком импорта модулей. Isort анализирует импорты в коде и автоматически сортирует их в соответствии с определенными правилами, такими как сортировка по имени модуля или по группам.

Pyupgrade представляет собой программный инструмент, предназначенный для автоматического обновления кода на языке Python до более новых версий данного языка. Такое обновление обеспечивает возможность использования новейших функциональных возможностей и улучшений, внедренных в последних версиях Python, а также элиминирует устаревшие конструкции и библиотеки. Путем проведения анализа исходного кода, Pyupgrade осуществляет автоматическое внесение изменений, таких как замена устаревших конструкций и актуализация синтаксиса, в соответствии с более современными стандартами языка.

Flake8 — это инструмент для обнаружения синтаксических и стилистических ошибок в коде на языке Python. Он выполняет статический анализ и проверяет код на соответствие стандартам кодирования и рекомендациям PEP 8. Flake8 выдает предупреждения и ошибки при обнаружении неправильного форматирования, несоответствия стилю кодирования или использования нежелательных конструкций. Это помогает улучшить качество кода, облегчает его понимание и уменьшает возможность возникновения ошибок.

Mypy представляет собой инструмент статического анализа кода на языке Python, призванный обнаруживать ошибки еще на этапе разработки. Путем проверки типов переменных, аргументов функций и других типовых аннотаций, указанных в исходном коде, Mypy способен выявить ошибки и предупредить об их наличии в случае несоответствия типов данных ожидаемым. Это позволяет предотвратить множество ошибок, связанных с типизацией данных, еще до запуска программы, что повышает надежность кода и облегчает его поддержку.

Также для автоматического форматирования кода использовался инструмент pre-commit, который позволяет форматировать кодовую базу перед отправкой в репозиторий с кодом.
** Тестирование системы
Для тестирования системы были написаны скрипты Github actions, которые запускали тестирование системы при отправки кода в репозиторий. Для тестирования использовалась библиотека pytest [cite:@krekel_pytest_2004], также для анализа покрытия кода использовалась библиотека =pytest-cov=.
*** Тестирование модуля работы с базой данных
При запуске каждого теста создавалась новая база данных и к ней применялись миграции. В зависимости от нужного теста в базу данных помещались нужные данные. Каждый метод API тестировался на верные данные, не верные данные и пустые данные, затем проверялось, что записалось в базу данных. В зависимости от требований каждого теста в базу данных помещались соответствующие данные. Затем создавался клиент для тестирования API с помощью библиотеки httpx.

Для каждого метода API тестировалось его поведение на правильные, неправильные и пустые данные. Затем проверялось, что записалось в базу данных и соответствует ожиданиям.

В результате было достигнуто 80% покрытия кода тестами (см. рисунок [[ref:fig:api_coverage]]).

#+CAPTION: Покрытие кода модуля работы с базой данных тестами
#+NAME: fig:api_coverage
#+ATTR_LATEX: :placement [h] :width 0.8\textwidth
[[file:img/api_coverage.png]]
*** Тестирование модуля сбора данных
Тестирование сборщиков данных заключается в проверке корректности извлечения и обработки данных, получаемых из HTML-страниц и json. Также важно проверять сбор данных на актуальных данных сайтов. Для имитации запросов к API и сайтам использовалась библиотека requests-mock, для сохранения данных страниц использовалась библиотека =vcrpy=. Данная библиотека при первом запросе сайта сохраняет данные запроса (заголовки, параметры и так далее), а тело запроса кодирует в байты, что позволяет сократить объем. При последующих запросов вместо реального ответа будут подставляться данные предыдущего ответа. Такой способ работы с запросами позволяет ускорить процесс тестирования. Также при запуске каждого теста создавалась новая база данных.

При написании тестирования были реализованы mock-данные для API. Для подмены запросов на тестовые сначала делались запросы (или брались данные предыдущих запросов), затем с помощью библиотеки =requests-mock= данные запроса подменялись на тестовые. Потом запускались тесты, которые проверяли корректности работы функций.

В результате было достигнуто 80% покрытия кода тестами (см. рисунок [[ref:fig:parser_coverage]]).

#+CAPTION: Покрытие кода модуля сборщиков данных тестами
#+NAME: fig:parser_coverage
#+ATTR_LATEX: :placement [h] :width 0.8\textwidth
[[file:img/parser_coverage.png]]
** Выводы по главе
В данной главе были описаны различные виды тестирования, которые использовались при тестировании модулей для работы базой данных и сбора данных.

Были описаны тесты для сборщиков данных, которые позволяют убедиться в корректности работы при различных сценариях. Также были рассмотрены тесты для HTTP API, которые проверяют работу API на правильную обработку запросов и на корректное взаимодействие с базой данных.
* Заключение
:PROPERTIES:
:UNNUMBERED: t
:END:
В ходе анализа предметной области, была рассмотрена проблема определения этичности компаний и был произведен обзор существующих решений, который показал, что сейчас нет индекса, который бы учитывал мнение клиентов для анализа этичности, и может потребоваться разработка нового средства, учитывающего особенности задачи.

Было произведено изучение существующих исследований и работ по обработке естественного языка и оно показало, что в этой сфере применяется множество алгоритмов, такие как мешок слов, TF-IDF, Word2Vec, FastText и BERT и каждый из этих алгоритмов показал свою эффективность в различных сферах. Также для задач классификации были рассмотрены: логистическая регрессия, метод опорных векторов, случайный лес и градиентный бустинг. И они были также рассмотрены в этой работе.

Был произведен анализ требований, который в результате которого были определены необходимые функциональные и нефункциональные требования, которые будут учитывались при разработке решения. В результате они были включены в состав технического задания для разрабатываемой системы.

В ходе проектирования системы она была разбита на отдельные модули и было описано их взаимодействие. Каждый модуль был разработан с учетом принципов микросервисной архитектуры и обеспечивает функциональность, необходимую для реализации системы в целом.

Была спроектирована база данных для хранения информации об отзывах, источниках, моделях и компаниях  с учетом требований к масштабируемости и производительности системы.

Было проведено проектирование методов для лучшего подбора метода обработки текста и модели классификации. Для этого была подобрана метрика, которая позволит лучшим образом отразить результат подбора моделей.

При реализации поставленной задачи было проведено обучение алгоритмов обработки естественного текста и классификации и лучший результат дало сочетание моделей дообученного BERT с размороженными двумя слоями и логистической регрессии с метрикой F1 равной 0.83.

Также было собрано 2,5 миллиона отзывов о компаниях с разных сайтов и реализованы модули по обработке и работе с данными. Система была развернута с помощью docker-compose.

Было проведено тестирование модулей сбора и работы с данными с результатом покрытия 80%.

В итоге можно заключить, что проект оказался успешным, поскольку были достигнуты все поставленные цели и выполнены поставленные задачи.

#+LATEX: \putbibliography
#+LATEX: \appendix
#+LATEX: \include{tz}
* Схема базы данных
#+begin_src d2 :exports results :file img/d2/database.png
company: {
  shape: sql_table

  id: integer {constraint: primary_key}
  company_name: varchar
  description: varchar
  company_type_id: integer {constraint: foreign_key}
  licence: varchar
}

company_type: {
  shape: sql_table

  id: integer {constraint: primary_key}
  name: varchar
}
model: {
  shape: sql_table

  id: integer {constraint: primary_key}
  name: varchar
  model_type_id: integer {constraint: foreign_key}
}
model_type: {
  shape: sql_table

  id: integer {constraint: primary_key}
  model_type: varchar
}
source: {
  shape: sql_table

  id: integer {constraint: primary_key}
  site: varchar
  source_type_id: integer {constraint: foreign_key}
  parser_state: JSON
  last_update: timestamp
}
source_type: {
  shape: sql_table

  id: integer {constraint: primary_key}
  name: varchar
}
text: {
  shape: sql_table

  id: integer {constraint: primary_key}
  link_: varchar
  source_id: integer {constraint: foreign_key}
  date: timestamp
  title: varchar
  bank_id: integer {constraint: foreign_key}
  comment_num: integer
}
text_result: {
  shape: sql_table

  id: integer {constraint: primary_key}
  text_sentence_id: integer {constraint: foreign_key}
  model_id: integer {constraint: foreign_key}
  result: double precision\[\]
  is_processed: boolean
}

text_sentence: {
  shape: sql_table

  id: integer {constraint: primary_key}
  text_id: integer {constraint: foreign_key}
  sentence: varchar
  sentence_num: integer
}

company -> company_type: company_type_id
model -> model_type: model_type_id
source -> source_type: source_type_id
text -> company: company_id
text -> source: source_id
text_result -> model: model_id
text_result -> text_sentence: text_sentence_id
text_sentence -> text: text_id
#+end_src

#+CAPTION: Схема базы данных
#+NAME: fig:database
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth

#+begin_src d2 :exports results :file img/d2/views_database.png
aggregate_table_model_result: {
  shape: sql_table

  id: integer {constraint: primary_key}
  year: integer
  quater: integer
  model_name: varchar
  source_site: varchar
  source_type: varchar
  company_name: varchar
  neutral: integer
  positive: integer
  negative: integer
  total: integer
  bank_id: integer
  index_base: double precision
  index_mean: double precision
  index_std: double precision
  index_safe: double precision
  # index_base_10_percentile: double precision
  # index_base_90_percentile: double precision
  # index_mean_10_percentile: double precision
  # index_mean_90_percentile: double precision
  # index_std_10_percentile: double precision
  # index_std_90_percentile: double precision
  # index_safe_10_percentile: double precision
  # index_safe_90_percentile: double precision
}

text_reviews_count: {
  shape: sql_table

  id: integer {constraint: primary_key}
  date: timestamp
  quarter: integer
  source_site: varchar
  source_type: varchar
  count_reviews: integer
}
#+end_src

#+CAPTION: Схема базы данных для агрегаций
#+NAME: fig:database_views
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
#+RESULTS:
[[file:img/d2/views_database.png]]

#+begin_src d2 :exports results :file img/d2/banki_ru.png
banki\.ru: {
  shape: sql_table

  id: integer {constraint: primary_key}
  company_id: integer
  company_name: varchar
  company_code: varchar
}
#+end_src

#+CAPTION: Схема базы данных сайта banki.ru
#+NAME: fig:database_banki_ru
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
#+RESULTS:
[[file:img/d2/banki_ru.png]]

#+begin_src d2 :exports results :file img/d2/sravni_ru.png
sravni\.ru: {
  shape: sql_table

  id: integer {constraint: primary_key}
  company_id: integer
  sravni_id: integer
  sravni_old_id: integer
  alias: varchar
  name: varchar
  full_name: varchar
  official_name: varchar
}
#+end_src

#+CAPTION: Схема базы данных сайта sravni.ru
#+NAME: fig:database_sravni_ru
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
#+RESULTS:
[[file:img/d2/sravni_ru.png]]

#+begin_src d2  :exports results :file img/d2/vk_com.png
vk\.com: {
  shape: sql_table

  id: integer {constraint: primary_key}
  vk_id: integer
  name: varchar
  domain: varchar
}
#+end_src

#+CAPTION: Схема базы данных сайта vk.com
#+NAME: fig:database_vk_com
#+ATTR_LATEX: :placement [h!] :width 0.8\textwidth
#+RESULTS:
[[file:img/d2/vk_com.png]]

#+begin_src mermaid :exports none
classDiagram
direction BT
class aggregate_table_model_result {
   integer year
   integer quater
   varchar model_name
   varchar source_site
   varchar source_type
   varchar bank_name
   integer neutral
   integer positive
   integer negative
   integer total
   integer bank_id
   double precision index_base
   double precision index_mean
   double precision index_std
   double precision index_safe
   double precision index_base_10_percentile
   double precision index_base_90_percentile
   double precision index_mean_10_percentile
   double precision index_mean_90_percentile
   double precision index_std_10_percentile
   double precision index_std_90_percentile
   double precision index_safe_10_percentile
   double precision index_safe_90_percentile
   integer id
}
class alembic_version {
   varchar(32) version_num
}
class bank {
   varchar bank_name
   varchar description
   integer bank_type_id
   varchar licence
   integer id
}
class bank_type {
   varchar name
   integer id
}
class model {
   varchar name
   integer model_type_id
   integer id
}
class model_type {
   varchar model_type
   integer id
}
class source {
   varchar site
   integer source_type_id
   varchar parser_state
   timestamp last_update
   integer id
}
class source_type {
   varchar name
   integer id
}
class text {
   varchar link
   integer source_id
   timestamp date
   varchar title
   integer bank_id
   integer comment_num
   integer id
}
class text_result {
   integer text_sentence_id
   integer model_id
   double precision[] result
   boolean is_processed
   integer id
}
class text_reviews_count {
   timestamp date
   integer quarter
   varchar source_site
   varchar source_type
   integer count_reviews
   integer id
}
class text_sentence {
   integer text_id
   varchar sentence
   integer sentence_num
   integer id
}

aggregate_table_model_result  -->  bank : bank_id:id
bank  -->  bank_type : bank_type_id:id
model  -->  model_type : model_type_id:id
source  -->  source_type : source_type_id:id
text  -->  bank : bank_id:id
text  -->  source : source_id:id
text_result  -->  model : model_id:id
text_result  -->  text_sentence : text_sentence_id:id
text_sentence  -->  text : text_id:id
#+end_src

* Запросы API
#+LATEX: \include{api_table}

#+LATEX: \begin{landscape}
* Диаграмма классов
#+LATEX: \thispagestyle{empty}
#+begin_src d2 :exports none :file img/d2/parser_class_v2.png
BaseParser: {
  shape: class

  parse()
  get_source_params(source Source): "tuple[int, int, datetime]"
}

BankiBase: {
  shape: class

  create_source(): Source
  load_bank_list()
  get_pages_num_html(url str, params dict\[str, Any\] \| None): "int | None"
  parse()
  get_pages_num(bank BankiRuBase): "int | None"
  get_page_bank_reviews(bank BankiRuBase, page_num int, parsed_time datetime): "list[Text] | None"
  get_reviews_from_url(url str, bank BankiRuBase, parsed_time datetime, params dict\[str, Any\] \| None): "list[Text]"
}

BankiBroker: {
  shape: class

  get_broker_licence_from_url(url str): "str | None"
  load_bank_list()
  get_page_bank_reviews(bank BankiRuBase, page_num int, parsed_time datetime): "list[Text] | None"
  get_pages_num(bank BankiRuBase): "int | None"
}

BankiInsurance: {
  shape: class

  get_pages_num_insurance_list(url str): int
  load_bank_list()
  get_pages_num(bank BankiRuBase): "int | None"
  get_page_bank_reviews(bank BankiRuBase, page_num int, parsed_time datetime): "list[Text] | None"
}

MfoParser: {
  shape: class

  get_mfo_json(page int): "dict[str, Any] | None"
  get_mfo_json_reviews(bank BankiRuBase, page int): "dict[str, Any] | None"
  json_total_pages(response dict\[str, Any\]): int
  get_microfin_list(): "list[BankiRuMfoScheme]"
  load_bank_list()
  get_page_bank_reviews(bank BankiRuBase, page_num int, parsed_time datetime): "list[Text] | None"
  get_pages_num(bank BankiRuBase): "int | None"
}

BankiNewsParser: {
  shape: class

  get_pages_num(bank BankiRuBase): "int | None"
  bank_news_page(bank BankiRuBase, page int = 1): "BeautifulSoup | None"
  get_news_links(bank BankiRuBase, parsed_time datetime, page_num int = 1): "list[str]"
  news_from_links(bank BankiRuBase, news_urls list\[str\]): "list[Text]"
  get_page_bank_reviews(bank BankiRuBase, page_num int, parsed_time datetime): "list[Text]"
}

BankiParser: {
  shape: class

  load_bank_list()
  get_page_bank_reviews(bank BankiRuBase, page_num int, parsed_time datetime): "list[Text] | None"
  get_pages_num(bank BankiRuBase): "int | None"
}

BaseSravni: {
  shape: class

  request_bank_list(): "dict[str, Any] | None"
  get_bank_reviews(bank_info SravniBankInfo, page_num int, page_size int): "dict[str, Any] | None"
  get_num_reviews(bank_info SravniBankInfo): int
  load_bank_list()
  parse_reviews(reviews_array list\[dict\[str, str\]\], last_date datetime, bank SravniBankInfo): "list[Text]"
  get_reviews(parsed_time datetime, bank_info SravniBankInfo): "list[Text]"
  get_review_link(bank_info SravniBankInfo, review dict\[str, Any\]): str
  parse()
  width: 1349
  height: 460
}

SravniReviews: {
  shape: class

  load_bank_list()
  get_review_link(bank_info SravniBankInfo, review dict\[str, Any\]): "str"
}

SravniInsuranceReviews: {
  shape: class

  load_bank_list()
  get_review_link(bank_info SravniBankInfo, review dict\[str, Any\]): str
}

SravniMfoReviews: {
  shape: class

  load_bank_list()
  get_review_link(bank_info SravniBankInfo, review dict\[str, Any\]): str
}

VKParser: {
  shape: class

  load_bank_list()
  json_to_comment_text(domain str, comment dict\[str, Any\], bank_id int, is_thread bool = False): Text
  get_post_comments(domain str, owner_id str, post_id str, comments_num int, bank_id int): "list[Text]"
  get_vk_source_params(source Source): "tuple[int, int, int, datetime]"
  parse()
  width: 1286
  height: 322
}

BaseParser -> BankiBase
BaseParser -> VKParser
BaseParser -> BaseSravni

BankiBase -> BankiBroker
BankiBase -> BankiInsurance
BankiBase -> MfoParser
BankiBase -> BankiParser
BankiParser -> BankiNewsParser

BaseSravni -> SravniInsuranceReviews
BaseSravni -> SravniMfoReviews
BaseSravni -> SravniReviews
BaseSravni -> SravniMfoReviews
#+end_src

#+CAPTION: Схема классов сборщиков данных
#+NAME: fig:parser_class_diagram
#+ATTR_LATEX: :placement [h!]
[[file:img/d2/parser_class.png]]
#+LATEX: \end{landscape}
* Листинг программы
В данном документе представлено описание структуры репозитория, в котором находится исходный код системы, описанной и разработанной в работе.

Репозиторий программы находится по ссылке: [[https://github.com/Samoed/EthicsAnalysis]].
* Акт о внедрении
#+LATEX: \hspace*{-2cm}\includegraphics[scale=0.85]{img/deployment_act.pdf}
* Footnotes
[fn:1] https://kontur.ru/expert, https://www.esphere.ru/products/spk/financial
[fn:2] https://proverki.gov.ru/portal/public-search

# Local Variables:
# org-latex-listings: t
# org-latex-default-packages-alist: nil
# End:
